{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c59b0e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up libraries\n",
    "import numpy as np\n",
    "import pdb as pdb\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import time\n",
    "import rioxarray as rxr\n",
    "import pandas as pd\n",
    "import math\n",
    "import h5py\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, mean_squared_error\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd30bd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global constants\n",
    "\n",
    "air_index_of_refraction = 1.0003\n",
    "c = 2.99792458e8   \n",
    "number_of_waveform_files = 100000\n",
    "\n",
    "# Set the WGS84 ellipsoid to NAD83 Geoid height offset for LVIS and GEDI\n",
    "ellipsoid_to_geoid_offset = 29.2548 # Harvard Forest, MA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c9c0325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define AOP pulsewaves directory, file, and constants\n",
    "\n",
    "aop_data_dir = '/Users/felixyu/Documents/Remote-Sensing/Neon-Pls-Wvs-Las/data/'\n",
    "waveform_file = aop_data_dir + 'NEON_D01_HARV_DP1_L022-1_2019082013_1.pls'\n",
    "# waveform_file = aop_data_dir + 'NEON_D01_HARV_DP1_L019-1_2019082013_1.pls'\n",
    "# waveform_file = aop_data_dir + 'NEON_D01_HARV_DP1_L018-1_2019082013_1.pls'\n",
    "# waveform_file = aop_data_dir + 'NEON_D01_HARV_DP1_L008-1_2019081112.plz'\n",
    "\n",
    "lidar_sensor_name = 'AOP'\n",
    "lidar_instrument_name_2019 = 'Gemini'\n",
    "lidar_instrument_name_2022 = 'LMS-Q780'\n",
    "lidar_instrument_name_2024 = 'Galaxy2024'\n",
    "\n",
    "# Define the waveform signal detection level - set by Dr. Keith Krause\n",
    "detection_threshold_2019 = 32.0\n",
    "detection_threshold_2022 = 8.0\n",
    "detection_threshold_2024 = 32.0 * 5.0\n",
    "\n",
    "# Set lidar instrument name and detection threshold based on current year of pulsewaves data\n",
    "lidar_instrument_name = lidar_instrument_name_2019\n",
    "detection_threshold = detection_threshold_2019\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eccb5add",
   "metadata": {},
   "outputs": [],
   "source": [
    "def waveform_peak_detection(waveform, waveform_intensity_threshold):\n",
    "    \"\"\"\n",
    "    A simple function to detect peaks in the waveform\n",
    "    Written by Dr. Keith Krause\n",
    "    \"\"\"\n",
    "    # Define x grid\n",
    "    x_grid = np.arange(0.0,len(waveform),1.0)\n",
    "\n",
    "    # Calculate 1st and 2nd derivatives\n",
    "    deriv_1st = np.gradient(waveform,axis=0,edge_order=1)\n",
    "    deriv_2nd = np.gradient(deriv_1st,axis=0,edge_order=1)\n",
    "\n",
    "    #Find locations where 1st derivative crosses 0: (deriv_1st[i] * deriv_1st[i-1] = negative value)\n",
    "    mult_shift_deriv_1st = deriv_1st * np.roll(deriv_1st,1)\n",
    "    zero_slope_aoi = np.argwhere(mult_shift_deriv_1st <= 0.0)\n",
    "    zero_slope_aoi = zero_slope_aoi[:,0]\n",
    "    zero_slope_count = len(zero_slope_aoi)\n",
    "    if zero_slope_count > 0:\n",
    "        zero_slope_aoi = np.reshape(zero_slope_aoi,len(zero_slope_aoi))\n",
    "\n",
    "    if zero_slope_count == 0:\n",
    "        return_location_count = 0\n",
    "        return_peak_location_list = -9999\n",
    "        return_location_list_x = -9999\n",
    "        return_intensity_list = -9999    \n",
    "    else:\n",
    "        if zero_slope_count == 1:    \n",
    "            if zero_slope_aoi == 0:\n",
    "                return_location_count = 0\n",
    "                return_peak_location_list = -9999\n",
    "                return_location_list_x = -9999\n",
    "                return_intensity_list = -9999\n",
    "            else:\n",
    "                if deriv_2nd[zero_slope_aoi] < 0.0:\n",
    "                    return_location_count = 1\n",
    "                    return_peak_location_list = zero_slope_aoi\n",
    "                    return_location_list_x = x_grid[zero_slope_aoi]\n",
    "                    return_intensity_list = waveform[zero_slope_aoi]               \n",
    "                else:\n",
    "                    return_location_count = 0\n",
    "                    return_peak_location_list = -9999\n",
    "                    return_location_list_x = -9999\n",
    "                    return_intensity_list = -9999    \n",
    "        else:\n",
    "                #Remove bin 0 if included in list\n",
    "            if zero_slope_aoi[0] == 0:\n",
    "                zero_slope_aoi = zero_slope_aoi[1:zero_slope_count]\n",
    "                zero_slope_count = zero_slope_count - 1        \n",
    "                #Remove sequential bins if included in list and keep first                \n",
    "            zero_slope_aoi_diff = zero_slope_aoi - np.roll(zero_slope_aoi,1)\n",
    "            good_zero_slope_aoi = np.argwhere(zero_slope_aoi_diff != 1)\n",
    "            good_zero_slope_aoi = np.reshape(good_zero_slope_aoi,len(good_zero_slope_aoi))\n",
    "            zero_slope_aoi = zero_slope_aoi[good_zero_slope_aoi]\n",
    "            zero_slope_count = len(good_zero_slope_aoi)\n",
    "            zero_slope_aoi_diff = 0\n",
    "            good_zero_slope_aoi = 0       \n",
    "                #Return peaks will have a negative 2nd derivative\n",
    "            peak_locations_aoi = np.argwhere(deriv_2nd[zero_slope_aoi] < 0.0)\n",
    "            return_location_count = len(peak_locations_aoi)\n",
    "            if return_location_count > 0:\n",
    "                peak_locations_aoi = peak_locations_aoi[:,0]\n",
    "                peak_locations_aoi = np.reshape(peak_locations_aoi,len(peak_locations_aoi))\n",
    "                return_peak_location_list = zero_slope_aoi[peak_locations_aoi]\n",
    "                return_location_list_x = x_grid[zero_slope_aoi[peak_locations_aoi]]\n",
    "                return_intensity_list = waveform[return_peak_location_list]\n",
    "            else:\n",
    "                return_peak_location_list = -9999\n",
    "                return_location_list_x = -9999\n",
    "                return_intensity_list = -9999\n",
    "            peak_locations_aoi = 0\n",
    "\n",
    "    if return_location_count > 0:\n",
    "        return_peak_location_list = np.reshape(return_peak_location_list,return_location_count)\n",
    "        return_location_list_x = np.reshape(return_location_list_x,return_location_count)\n",
    "        return_intensity_list = np.reshape(return_intensity_list,return_location_count)\n",
    "                     \n",
    "    # Remove peaks with peak location at first 2 bins\n",
    "    if return_location_count > 0:\n",
    "        good_return_aoi = np.argwhere(return_peak_location_list > 1)\n",
    "        good_return_count = len(good_return_aoi)\n",
    "        if good_return_count > 0:\n",
    "            good_return_aoi = np.reshape(good_return_aoi,good_return_count)\n",
    "            return_location_count = good_return_count\n",
    "            return_peak_location_list = return_peak_location_list[good_return_aoi]\n",
    "            return_location_list_x = return_location_list_x[good_return_aoi]\n",
    "            return_intensity_list = return_intensity_list[good_return_aoi]\n",
    "        else:\n",
    "            return_location_count = 0\n",
    "            return_peak_location_list = -9999\n",
    "            return_location_list_x = -9999\n",
    "            return_intensity_list = -9999        \n",
    "        good_return_aoi = 0                    \n",
    "\n",
    "    # Check intensity values for valid peaks\n",
    "    if return_location_count > 0:\n",
    "        good_return_aoi = np.argwhere(return_intensity_list >= waveform_intensity_threshold)\n",
    "        good_return_count = len(good_return_aoi)\n",
    "        if good_return_count > 0:\n",
    "            good_return_aoi = np.reshape(good_return_aoi,good_return_count)\n",
    "            return_location_count = good_return_count\n",
    "            return_peak_location_list = return_peak_location_list[good_return_aoi]\n",
    "            return_location_list_x = return_location_list_x[good_return_aoi]\n",
    "            return_intensity_list = return_intensity_list[good_return_aoi]\n",
    "        else:\n",
    "            return_location_count = 0\n",
    "            return_peak_location_list = -9999\n",
    "            return_location_list_x = -9999\n",
    "            return_intensity_list = -9999        \n",
    "        good_return_aoi = 0    \n",
    "    \n",
    "    x_grid = 0\n",
    "    deriv_1st = 0\n",
    "    deriv_2nd = 0\n",
    "    mult_shift_deriv_1st = 0\n",
    "    zero_slope_aoi = 0\n",
    "    waveform = 0\n",
    "    \n",
    "    return return_location_count, return_peak_location_list, return_location_list_x, return_intensity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0866b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_NEONAOP_pulsewaves_pulse_information(pls_file, lidar_instrument_name, number_of_waveform_files=number_of_waveform_files):\n",
    "    \"\"\"\n",
    "    Reads PulseWaves (PLS) file metadata, pulse information and geolocation data for a specified number of pulsewaves, returning\n",
    "    instrument name, number of pulses, anchor positions, wave offsets, geolocation data, etc.\n",
    "    Written by Dr. Keith Krause with modifications by Felix Yu\n",
    "    \"\"\"\n",
    "\n",
    "    readbin_pls_file = open(pls_file,\"rb\")\n",
    "    readbin_pls_file.seek(0,0)\n",
    "    \n",
    "    # Read PulseWaves Header\n",
    "    file_signature = np.fromfile(readbin_pls_file,np.int8,16)\n",
    "    global_parameters = np.ndarray.item(np.fromfile(readbin_pls_file,np.uint32,1))\n",
    "    file_source_id = np.ndarray.item(np.fromfile(readbin_pls_file,np.uint32,1))\n",
    "    project_id_guid_data1 = np.ndarray.item(np.fromfile(readbin_pls_file,np.uint32,1))\n",
    "    project_id_guid_data2 = np.ndarray.item(np.fromfile(readbin_pls_file,np.uint16,1))\n",
    "    project_id_guid_data3 = np.ndarray.item(np.fromfile(readbin_pls_file,np.uint16,1))\n",
    "    project_id_guid_data4 = np.fromfile(readbin_pls_file,np.uint8,8)\n",
    "    system_identifier = np.fromfile(readbin_pls_file,np.int8,64)    \n",
    "    generating_software = np.fromfile(readbin_pls_file,np.int8,64)\n",
    "    file_creation_day_of_year = np.ndarray.item(np.fromfile(readbin_pls_file,np.uint16,1))\n",
    "    file_creation_year = np.ndarray.item(np.fromfile(readbin_pls_file,np.uint16,1))\n",
    "    version_major = np.ndarray.item(np.fromfile(readbin_pls_file,np.uint8,1))\n",
    "    version_minor = np.ndarray.item(np.fromfile(readbin_pls_file,np.uint8,1))\n",
    "    header_size = np.ndarray.item(np.fromfile(readbin_pls_file,np.uint16,1))\n",
    "    offset_to_pulse_data = np.ndarray.item(np.fromfile(readbin_pls_file,np.int64,1))\n",
    "    number_of_pulses = np.ndarray.item(np.fromfile(readbin_pls_file,np.int64,1))\n",
    "    pulse_format = np.ndarray.item(np.fromfile(readbin_pls_file,np.uint32,1))\n",
    "    pulse_attributes = np.ndarray.item(np.fromfile(readbin_pls_file,np.uint32,1))\n",
    "    pulse_size = np.ndarray.item(np.fromfile(readbin_pls_file,np.uint32,1))\n",
    "    pulse_compression = np.ndarray.item(np.fromfile(readbin_pls_file,np.uint32,1))\n",
    "    reserved = np.ndarray.item(np.fromfile(readbin_pls_file,np.int64,1))\n",
    "    number_of_variable_length_records = np.ndarray.item(np.fromfile(readbin_pls_file,np.uint32,1))\n",
    "    number_of_appended_variable_length_records = np.ndarray.item(np.fromfile(readbin_pls_file,np.int32,1))\n",
    "    T_scale_factor = np.ndarray.item(np.fromfile(readbin_pls_file,np.double,1))\n",
    "    T_offset = np.ndarray.item(np.fromfile(readbin_pls_file,np.double,1))\n",
    "    min_T = np.ndarray.item(np.fromfile(readbin_pls_file,np.int64,1))\n",
    "    max_T = np.ndarray.item(np.fromfile(readbin_pls_file,np.int64,1))\n",
    "    x_scale_factor = np.ndarray.item(np.fromfile(readbin_pls_file,np.double,1))\n",
    "    y_scale_factor = np.ndarray.item(np.fromfile(readbin_pls_file,np.double,1))\n",
    "    z_scale_factor = np.ndarray.item(np.fromfile(readbin_pls_file,np.double,1))\n",
    "    x_offset = np.ndarray.item(np.fromfile(readbin_pls_file,np.double,1))\n",
    "    y_offset = np.ndarray.item(np.fromfile(readbin_pls_file,np.double,1))\n",
    "    z_offset = np.ndarray.item(np.fromfile(readbin_pls_file,np.double,1))\n",
    "    min_x = np.ndarray.item(np.fromfile(readbin_pls_file,np.double,1))\n",
    "    max_x = np.ndarray.item(np.fromfile(readbin_pls_file,np.double,1))\n",
    "    min_y = np.ndarray.item(np.fromfile(readbin_pls_file,np.double,1))\n",
    "    max_y = np.ndarray.item(np.fromfile(readbin_pls_file,np.double,1))\n",
    "    min_z = np.ndarray.item(np.fromfile(readbin_pls_file,np.double,1))\n",
    "    max_z = np.ndarray.item(np.fromfile(readbin_pls_file,np.double,1))\n",
    "\n",
    "    # Read variable length records\n",
    "    vlr_user_id = []\n",
    "    vlr_record_id = []\n",
    "    vlr_reserved = []\n",
    "    vlr_record_length_after_header = []\n",
    "    vlr_description = []\n",
    "    vlr_data = []\n",
    "    pulse_descriptor_size = [] \n",
    "    pulse_descriptor_reserved = [] \n",
    "    pulse_descriptor_optical_center_to_anchor_point = []\n",
    "    pulse_descriptor_number_of_extra_wave_bytes = []\n",
    "    pulse_descriptor_number_of_samplings = [] \n",
    "    pulse_descriptor_sample_units = [] \n",
    "    pulse_descriptor_compression = [] \n",
    "    pulse_descriptor_scanner_index = [] \n",
    "    pulse_descriptor_description = []\n",
    "    sampling_record_size = []\n",
    "    sampling_record_reserved = []\n",
    "    sampling_record_type = []\n",
    "    sampling_record_channel = []\n",
    "    sampling_record_unused = []\n",
    "    sampling_record_bits_for_duration_from_anchor = []\n",
    "    sampling_record_scale_for_duration_from_anchor = []\n",
    "    sampling_record_offset_for_duration_from_anchor = []\n",
    "    sampling_record_bits_for_number_of_segments = []\n",
    "    sampling_record_bits_for_number_of_samples = []\n",
    "    sampling_record_number_of_segments = []\n",
    "    sampling_record_number_of_samples = []\n",
    "    sampling_record_bits_per_sample = []\n",
    "    sampling_record_lookup_table_index = []\n",
    "    sampling_record_sample_units = []\n",
    "    sampling_record_compression = []\n",
    "    sampling_record_description = []\n",
    "    num_pulse_descriptor_records = 0\n",
    "    sampling_record_pulse_descriptor_index_lookup = []\n",
    "\n",
    "    wv_count = 0\n",
    "\n",
    "    for i in range(0,int(number_of_variable_length_records)):\n",
    "        vlr_user_id.append(np.fromfile(readbin_pls_file,np.int8,16))\n",
    "        vlr_record_id.append(np.ndarray.item(np.fromfile(readbin_pls_file,np.uint32,1)))\n",
    "        vlr_reserved.append(np.ndarray.item(np.fromfile(readbin_pls_file,np.uint32,1)))\n",
    "        vlr_record_length_after_header.append(np.ndarray.item(np.fromfile(readbin_pls_file,np.int64,1)))\n",
    "        vlr_description.append(np.fromfile(readbin_pls_file,np.int8,64))\n",
    "\n",
    "        if(vlr_record_id[i] >= 100001 and vlr_record_id[i] < 100255):\n",
    "            scanner_size = np.ndarray.item(np.fromfile(readbin_pls_file,np.uint32,1))\n",
    "            scanner_reserved = np.ndarray.item(np.fromfile(readbin_pls_file,np.uint32,1))\n",
    "            scanner_instrument = np.fromfile(readbin_pls_file,np.int8,64)\n",
    "            scanner_serial = np.fromfile(readbin_pls_file,np.int8,64)\n",
    "            scanner_wave_length = np.ndarray.item(np.fromfile(readbin_pls_file,np.float32,1))\n",
    "            scanner_outgoing_pulse_width = np.ndarray.item(np.fromfile(readbin_pls_file,np.float32,1))\n",
    "            scanner_scan_pattern = np.ndarray.item(np.fromfile(readbin_pls_file,np.uint32,1))\n",
    "            scanner_number_of_mirror_facets = np.ndarray.item(np.fromfile(readbin_pls_file,np.uint32,1))\n",
    "            scanner_scan_frequency = np.ndarray.item(np.fromfile(readbin_pls_file,np.float32,1))\n",
    "            scanner_scan_angle_min = np.ndarray.item(np.fromfile(readbin_pls_file,np.float32,1))\n",
    "            scanner_scan_angle_max = np.ndarray.item(np.fromfile(readbin_pls_file,np.float32,1))\n",
    "            scanner_pulse_frequency = np.ndarray.item(np.fromfile(readbin_pls_file,np.float32,1))\n",
    "            scanner_beam_diameter_at_exit_apertrue = np.ndarray.item(np.fromfile(readbin_pls_file,np.float32,1))\n",
    "            scanner_beam_divergence = np.ndarray.item(np.fromfile(readbin_pls_file,np.float32,1))\n",
    "            scanner_minimal_range = np.ndarray.item(np.fromfile(readbin_pls_file,np.float32,1))\n",
    "            scanner_maximal_range = np.ndarray.item(np.fromfile(readbin_pls_file,np.float32,1))\n",
    "            scanner_description = np.fromfile(readbin_pls_file,np.int8,64)\n",
    "            continue    \n",
    "\n",
    "        if (vlr_record_id[i] >= 20001 and vlr_record_id[i] < 200255):\n",
    "            pulse_descriptor_index = vlr_record_id[i] - 200000\n",
    "            pulse_descriptor_size.append(np.ndarray.item(np.fromfile(readbin_pls_file,np.uint32,1)))\n",
    "            pulse_descriptor_reserved.append(np.ndarray.item(np.fromfile(readbin_pls_file,np.uint32,1)))\n",
    "            pulse_descriptor_optical_center_to_anchor_point.append(np.ndarray.item(np.fromfile(readbin_pls_file,np.int32,1)))\n",
    "            pulse_descriptor_number_of_extra_wave_bytes.append(np.ndarray.item(np.fromfile(readbin_pls_file,np.uint16,1)))\n",
    "            pulse_descriptor_number_of_samplings.append(np.ndarray.item(np.fromfile(readbin_pls_file,np.uint16,1)))\n",
    "            pulse_descriptor_sample_units.append(np.ndarray.item(np.fromfile(readbin_pls_file,np.float32,1)))\n",
    "            pulse_descriptor_compression.append(np.ndarray.item(np.fromfile(readbin_pls_file,np.uint32,1)))\n",
    "            pulse_descriptor_scanner_index.append(np.ndarray.item(np.fromfile(readbin_pls_file,np.uint32,1)))\n",
    "            pulse_descriptor_description1 = np.fromfile(readbin_pls_file,np.int8,64)\n",
    "            pulse_descriptor_description.append(pulse_descriptor_description1)\n",
    "            num_pulse_descriptor_records = num_pulse_descriptor_records + 1\n",
    "\n",
    "            for j in range(0,int(pulse_descriptor_number_of_samplings[-1])):\n",
    "                sampling_record_pulse_descriptor_index_lookup.append(pulse_descriptor_index)\n",
    "                sampling_record_size.append(np.ndarray.item(np.fromfile(readbin_pls_file,np.uint32,1)))\n",
    "                sampling_record_reserved.append(np.ndarray.item(np.fromfile(readbin_pls_file,np.uint32,1)))\n",
    "                sampling_record_type.append(np.ndarray.item(np.fromfile(readbin_pls_file,np.uint8,1)))\n",
    "                sampling_record_channel.append(np.ndarray.item(np.fromfile(readbin_pls_file,np.uint8,1)))\n",
    "                sampling_record_unused.append(np.ndarray.item(np.fromfile(readbin_pls_file,np.uint8,1)))\n",
    "                sampling_record_bits_for_duration_from_anchor.append(np.ndarray.item(np.fromfile(readbin_pls_file,np.uint8,1)))\n",
    "                sampling_record_scale_for_duration_from_anchor.append(np.ndarray.item(np.fromfile(readbin_pls_file,np.float32,1)))\n",
    "                sampling_record_offset_for_duration_from_anchor.append(np.ndarray.item(np.fromfile(readbin_pls_file,np.float32,1)))\n",
    "                sampling_record_bits_for_number_of_segments.append(np.ndarray.item(np.fromfile(readbin_pls_file,np.uint8,1)))\n",
    "                sampling_record_bits_for_number_of_samples.append(np.ndarray.item(np.fromfile(readbin_pls_file,np.uint8,1)))\n",
    "                sampling_record_number_of_segments.append(np.ndarray.item(np.fromfile(readbin_pls_file,np.uint16,1)))\n",
    "                sampling_record_number_of_samples.append(np.ndarray.item(np.fromfile(readbin_pls_file,np.uint32,1)))\n",
    "                sampling_record_bits_per_sample.append(np.ndarray.item(np.fromfile(readbin_pls_file,np.uint16,1)))\n",
    "                sampling_record_lookup_table_index.append(np.ndarray.item(np.fromfile(readbin_pls_file,np.uint16,1)))\n",
    "                sampling_record_sample_units.append(np.ndarray.item(np.fromfile(readbin_pls_file,np.float32,1)))\n",
    "                sampling_record_compression.append(np.ndarray.item(np.fromfile(readbin_pls_file,np.uint32,1)))\n",
    "                sampling_record_description1 = np.fromfile(readbin_pls_file,np.int8,64)\n",
    "                sampling_record_description.append(sampling_record_description1)\n",
    "            continue\n",
    "\n",
    "        vlr_data.append(np.fromfile(readbin_pls_file,np.uint8,int(vlr_record_length_after_header[i])))\n",
    "\n",
    "    # Reshape pulse descriptor and sampling record arrays\n",
    "    # pulse_descriptor_description and sampling_record_description not included\n",
    "    pulse_descriptor_size_array = np.reshape(pulse_descriptor_size,len(pulse_descriptor_size))\n",
    "    pulse_descriptor_reserved_array = np.reshape(pulse_descriptor_reserved,len(pulse_descriptor_reserved))\n",
    "    pulse_descriptor_optical_center_to_anchor_point_array = np.reshape(pulse_descriptor_optical_center_to_anchor_point,len(pulse_descriptor_optical_center_to_anchor_point))\n",
    "    pulse_descriptor_number_of_extra_wave_bytes_array = np.reshape(pulse_descriptor_number_of_extra_wave_bytes,len(pulse_descriptor_number_of_extra_wave_bytes))\n",
    "    pulse_descriptor_number_of_samplings_array = np.reshape(pulse_descriptor_number_of_samplings,len(pulse_descriptor_number_of_samplings))\n",
    "    pulse_descriptor_sample_units_array = np.reshape(pulse_descriptor_sample_units,len(pulse_descriptor_sample_units))\n",
    "    pulse_descriptor_compression_array = np.reshape(pulse_descriptor_compression,len(pulse_descriptor_compression))\n",
    "    pulse_descriptor_scanner_index_array = np.reshape(pulse_descriptor_scanner_index,len(pulse_descriptor_scanner_index))\n",
    "    sampling_record_pulse_descriptor_index_lookup_array = np.reshape(sampling_record_pulse_descriptor_index_lookup,len(sampling_record_pulse_descriptor_index_lookup))                               \n",
    "    sampling_record_size_array = np.reshape(sampling_record_size,len(sampling_record_size))                                                                \n",
    "    sampling_record_reserved_array = np.reshape(sampling_record_reserved,len(sampling_record_reserved))                                                                 \n",
    "    sampling_record_type_array = np.reshape(sampling_record_type,len(sampling_record_type))                                                                 \n",
    "    sampling_record_channel_array = np.reshape(sampling_record_channel,len(sampling_record_channel))                                                                 \n",
    "    sampling_record_unused_array = np.reshape(sampling_record_unused,len(sampling_record_unused))                                                                 \n",
    "    sampling_record_bits_for_duration_from_anchor_array = np.reshape(sampling_record_bits_for_duration_from_anchor,len(sampling_record_bits_for_duration_from_anchor))                                                               \n",
    "    sampling_record_scale_for_duration_from_anchor_array = np.reshape(sampling_record_scale_for_duration_from_anchor,len(sampling_record_scale_for_duration_from_anchor))\n",
    "    sampling_record_offset_for_duration_from_anchor_array = np.reshape(sampling_record_offset_for_duration_from_anchor,len(sampling_record_offset_for_duration_from_anchor))\n",
    "    sampling_record_bits_for_number_of_segments_array = np.reshape(sampling_record_bits_for_number_of_segments,len(sampling_record_bits_for_number_of_segments))\n",
    "    sampling_record_bits_for_number_of_samples_array = np.reshape(sampling_record_bits_for_number_of_samples,len(sampling_record_bits_for_number_of_samples))\n",
    "    sampling_record_number_of_segments_array = np.reshape(sampling_record_number_of_segments,len(sampling_record_number_of_segments))\n",
    "    sampling_record_number_of_samples_array = np.reshape(sampling_record_number_of_samples,len(sampling_record_number_of_samples))\n",
    "    sampling_record_bits_per_sample_array = np.reshape(sampling_record_bits_per_sample,len(sampling_record_bits_per_sample))\n",
    "    sampling_record_lookup_table_index_array = np.reshape(sampling_record_lookup_table_index,len(sampling_record_lookup_table_index))\n",
    "    sampling_record_sample_units_array = np.reshape(sampling_record_sample_units,len(sampling_record_sample_units))\n",
    "    sampling_record_compression_array = np.reshape(sampling_record_compression,len(sampling_record_compression))    \n",
    "    \n",
    "    instrument_name = str(scanner_instrument,'UTF-8')\n",
    "    \n",
    "    if (instrument_name[0:6] == 'Galaxy'):\n",
    "        instrument_name = instrument_name[0:6]\n",
    "    if (instrument_name[0:11] == 'ALTM GEMINI'):\n",
    "        instrument_name = 'Gemini'\n",
    "    if (instrument_name[5:9] == 'Q780'):\n",
    "        instrument_name = 'LMS-Q780'\n",
    "    if (instrument_name[0:4] == 'Auto'):\n",
    "        instrument_name = lidar_instrument_name #'Galaxy' #'LMS-Q780'\n",
    "    \n",
    "    #if Optech T_offset = 0\n",
    "    if ((instrument_name == 'Gemini') | (instrument_name == 'Galaxy')):\n",
    "        T_offset = 0.0\n",
    "\n",
    "    print('instrument_name = ', instrument_name)\n",
    "        \n",
    "    # Loop through PLS file and get info for all pulses\n",
    "    gps_timestamp_T_array = np.zeros(number_of_pulses)\n",
    "    offset_to_waves_array = np.zeros(number_of_pulses)\n",
    "    anchor_X_array = np.zeros(number_of_pulses)\n",
    "    anchor_Y_array = np.zeros(number_of_pulses)  \n",
    "    anchor_Z_array = np.zeros(number_of_pulses)\n",
    "    target_X_array = np.zeros(number_of_pulses)    \n",
    "    target_Y_array = np.zeros(number_of_pulses)\n",
    "    target_Z_array = np.zeros(number_of_pulses)\n",
    "    first_returning_sample_array = np.zeros(number_of_pulses)\n",
    "    last_returning_sample_array = np.zeros(number_of_pulses)\n",
    "    pulse_data_array = np.zeros((np.longlong(number_of_pulses),2))\n",
    "    intensity_array = np.zeros(number_of_pulses)\n",
    "    classification_array = np.zeros(number_of_pulses)\n",
    "    gps_timestamp_array = np.zeros(number_of_pulses)\n",
    "    xyz_anchor_array = np.zeros((np.longlong(number_of_pulses),3))\n",
    "    xyz_target_array = np.zeros((np.longlong(number_of_pulses),3))   \n",
    "    pulse_descriptor_index_array = np.zeros(number_of_pulses)\n",
    "    dxdydz_array = np.zeros((np.longlong(number_of_pulses),3))   \n",
    "    xyz_first_array = np.zeros((np.longlong(number_of_pulses),3))   \n",
    "    xyz_last_array = np.zeros((np.longlong(number_of_pulses),3))   \n",
    "\n",
    "    # Read PLS Pulse Data\n",
    "    for iPulse in range(number_of_pulses):\n",
    "        pls_file_offset = np.int64(offset_to_pulse_data + iPulse * pulse_size)\n",
    "        readbin_pls_file.seek(pls_file_offset,0)\n",
    "        #read pulse record\n",
    "        gps_timestamp_T_array[iPulse] = np.ndarray.item(np.fromfile(readbin_pls_file,np.int64,1))\n",
    "        offset_to_waves_array[iPulse] = np.ndarray.item(np.fromfile(readbin_pls_file,np.int64,1))\n",
    "        anchor_X_array[iPulse] = np.ndarray.item(np.fromfile(readbin_pls_file,np.int32,1))\n",
    "        anchor_Y_array[iPulse] = np.ndarray.item(np.fromfile(readbin_pls_file,np.int32,1))   \n",
    "        anchor_Z_array[iPulse] = np.ndarray.item(np.fromfile(readbin_pls_file,np.int32,1))\n",
    "        target_X_array[iPulse] = np.ndarray.item(np.fromfile(readbin_pls_file,np.int32,1))    \n",
    "        target_Y_array[iPulse] = np.ndarray.item(np.fromfile(readbin_pls_file,np.int32,1))   \n",
    "        target_Z_array[iPulse] = np.ndarray.item(np.fromfile(readbin_pls_file,np.int32,1))   \n",
    "        first_returning_sample_array[iPulse] = np.ndarray.item(np.fromfile(readbin_pls_file,np.int16,1))  \n",
    "        last_returning_sample_array[iPulse] = np.ndarray.item(np.fromfile(readbin_pls_file,np.int16,1))  \n",
    "        pulse_data_array[iPulse,:] = np.fromfile(readbin_pls_file,np.int8,2)  \n",
    "        intensity_array[iPulse] = np.ndarray.item(np.fromfile(readbin_pls_file,np.int8,1))\n",
    "        classification_array[iPulse] = np.ndarray.item(np.fromfile(readbin_pls_file,np.int8,1))\n",
    "        gps_timestamp_array[iPulse] = np.double(gps_timestamp_T_array[iPulse]) * T_scale_factor + T_offset\n",
    "        xyz_anchor_array[iPulse,0] = np.double(anchor_X_array[iPulse]) * x_scale_factor + x_offset\n",
    "        xyz_anchor_array[iPulse,1] = np.double(anchor_Y_array[iPulse]) * y_scale_factor + y_offset\n",
    "        xyz_anchor_array[iPulse,2] = np.double(anchor_Z_array[iPulse]) * z_scale_factor + z_offset\n",
    "        xyz_target_array[iPulse,0] = np.double(target_X_array[iPulse]) * x_scale_factor + x_offset    \n",
    "        xyz_target_array[iPulse,1] = np.double(target_Y_array[iPulse]) * y_scale_factor + y_offset        \n",
    "        xyz_target_array[iPulse,2] = np.double(target_Z_array[iPulse]) * z_scale_factor + z_offset        \n",
    "        pulse_descriptor_index_array[iPulse] = pulse_data_array[iPulse,0]\n",
    "        dxdydz_array[iPulse,0] = (xyz_target_array[iPulse,0] - xyz_anchor_array[iPulse,0]) / 1000.0\n",
    "        dxdydz_array[iPulse,1] = (xyz_target_array[iPulse,1] - xyz_anchor_array[iPulse,1]) / 1000.0\n",
    "        dxdydz_array[iPulse,2] = (xyz_target_array[iPulse,2] - xyz_anchor_array[iPulse,2]) / 1000.0\n",
    "        xyz_first_array[iPulse,0] = xyz_anchor_array[iPulse,0] + np.double(first_returning_sample_array[iPulse]) * dxdydz_array[iPulse,0]\n",
    "        xyz_first_array[iPulse,1] = xyz_anchor_array[iPulse,1] + np.double(first_returning_sample_array[iPulse]) * dxdydz_array[iPulse,1]\n",
    "        xyz_first_array[iPulse,2] = xyz_anchor_array[iPulse,2] + np.double(first_returning_sample_array[iPulse]) * dxdydz_array[iPulse,2]\n",
    "        xyz_last_array[iPulse,0] = xyz_anchor_array[iPulse,0] + np.double(last_returning_sample_array[iPulse]) * dxdydz_array[iPulse,0]\n",
    "        xyz_last_array[iPulse,1] = xyz_anchor_array[iPulse,1] + np.double(last_returning_sample_array[iPulse]) * dxdydz_array[iPulse,1]\n",
    "        xyz_last_array[iPulse,2] = xyz_anchor_array[iPulse,2] + np.double(last_returning_sample_array[iPulse]) * dxdydz_array[iPulse,2]   \n",
    "        \n",
    "        # Process a certain number of pls files for speed\n",
    "        wv_count += 1\n",
    "        if wv_count > number_of_waveform_files:\n",
    "            break\n",
    "    \n",
    "    readbin_pls_file.close()\n",
    "\n",
    "    # Return instrument_name number of pulses and geolocation for anchor position, first and last waveform bins, and ray vector dxdydz, plus pulsewaves data\n",
    "    return instrument_name, number_of_pulses, xyz_anchor_array, dxdydz_array, xyz_first_array, xyz_last_array, offset_to_pulse_data, pulse_size, T_scale_factor, T_offset, x_scale_factor, x_offset, y_scale_factor, y_offset, z_scale_factor, z_offset, sampling_record_pulse_descriptor_index_lookup_array, pulse_descriptor_optical_center_to_anchor_point_array, pulse_descriptor_number_of_extra_wave_bytes_array, pulse_descriptor_number_of_samplings_array, sampling_record_bits_for_duration_from_anchor_array, sampling_record_scale_for_duration_from_anchor_array, sampling_record_offset_for_duration_from_anchor_array, sampling_record_bits_for_number_of_segments_array, sampling_record_bits_for_number_of_samples_array, sampling_record_number_of_segments_array, sampling_record_number_of_samples_array, sampling_record_bits_per_sample_array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d1bc780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to read a pulsewaves waveform for a single pulse\n",
    "def read_NEONAOP_pulsewaves_waveform(readbin_pls_file,readbin_wvs_file,iPulse,offset_to_pulse_data,pulse_size,T_scale_factor,T_offset,x_scale_factor,x_offset,y_scale_factor,y_offset,z_scale_factor,z_offset,sampling_record_pulse_descriptor_index_lookup_array,pulse_descriptor_optical_center_to_anchor_point_array,pulse_descriptor_number_of_extra_wave_bytes_array,pulse_descriptor_number_of_samplings_array,sampling_record_bits_for_duration_from_anchor_array,sampling_record_scale_for_duration_from_anchor_array,sampling_record_offset_for_duration_from_anchor_array,sampling_record_bits_for_number_of_segments_array,sampling_record_bits_for_number_of_samples_array,sampling_record_number_of_segments_array,sampling_record_number_of_samples_array,sampling_record_bits_per_sample_array):    \n",
    "    readbin_pls_file.seek(0,0)\n",
    "    readbin_wvs_file.seek(0,0)\n",
    "    #read pulse record\n",
    "    pls_file_offset = np.int64(offset_to_pulse_data + iPulse * pulse_size)\n",
    "    readbin_pls_file.seek(pls_file_offset[0],0)\n",
    "    gps_timestamp_T = np.fromfile(readbin_pls_file,np.int64,1)\n",
    "    offset_to_waves = np.fromfile(readbin_pls_file,np.int64,1)\n",
    "    anchor_X = np.fromfile(readbin_pls_file,np.int32,1)\n",
    "    anchor_Y = np.fromfile(readbin_pls_file,np.int32,1)    \n",
    "    anchor_Z = np.fromfile(readbin_pls_file,np.int32,1)\n",
    "    target_X = np.fromfile(readbin_pls_file,np.int32,1)    \n",
    "    target_Y = np.fromfile(readbin_pls_file,np.int32,1)   \n",
    "    target_Z = np.fromfile(readbin_pls_file,np.int32,1)   \n",
    "    first_returning_sample = np.fromfile(readbin_pls_file,np.int16,1)  \n",
    "    last_returning_sample = np.fromfile(readbin_pls_file,np.int16,1)  \n",
    "    pulse_data = np.fromfile(readbin_pls_file,np.int8,2)  \n",
    "    intensity = np.fromfile(readbin_pls_file,np.int8,1)\n",
    "    classification = np.fromfile(readbin_pls_file,np.int8,1)\n",
    "    \n",
    "    gps_timestamp = np.double(gps_timestamp_T) * T_scale_factor + T_offset\n",
    "    x_anchor = np.double(anchor_X) * x_scale_factor + x_offset\n",
    "    y_anchor = np.double(anchor_Y) * y_scale_factor + y_offset\n",
    "    z_anchor = np.double(anchor_Z) * z_scale_factor + z_offset\n",
    "    x_target = np.double(target_X) * x_scale_factor + x_offset    \n",
    "    y_target = np.double(target_Y) * y_scale_factor + y_offset        \n",
    "    z_target = np.double(target_Z) * z_scale_factor + z_offset        \n",
    "    pulse_descriptor_index = pulse_data[0]\n",
    "    dx = (x_target - x_anchor) / 1000.0\n",
    "    dy = (y_target - y_anchor) / 1000.0\n",
    "    dz = (z_target - z_anchor) / 1000.0\n",
    "    x_first = x_anchor + np.double(first_returning_sample) * dx\n",
    "    y_first = y_anchor + np.double(first_returning_sample) * dy\n",
    "    z_first = z_anchor + np.double(first_returning_sample) * dz\n",
    "    x_last = x_anchor + np.double(last_returning_sample) * dx\n",
    "    y_last = y_anchor + np.double(last_returning_sample) * dy\n",
    "    z_last = z_anchor + np.double(last_returning_sample) * dz      \n",
    "\n",
    "    #read waveforms\n",
    "    pulse_descriptor_aoi = np.argwhere(sampling_record_pulse_descriptor_index_lookup_array == pulse_descriptor_index)\n",
    "    pulse_descriptor_aoi = np.reshape(pulse_descriptor_aoi,len(pulse_descriptor_aoi))\n",
    "\n",
    "    optical_center_to_anchor_point = pulse_descriptor_optical_center_to_anchor_point_array[pulse_descriptor_index-1]\n",
    "    number_of_extra_wave_bytes = pulse_descriptor_number_of_extra_wave_bytes_array[pulse_descriptor_index-1]\n",
    "    number_of_samplings = pulse_descriptor_number_of_samplings_array[pulse_descriptor_index-1]\n",
    "\n",
    "    bits_for_duration_from_anchor_array = sampling_record_bits_for_duration_from_anchor_array[pulse_descriptor_aoi]\n",
    "    scale_for_duration_from_anchor_array = sampling_record_scale_for_duration_from_anchor_array[pulse_descriptor_aoi]\n",
    "    offset_for_duration_from_anchor_array = sampling_record_offset_for_duration_from_anchor_array[pulse_descriptor_aoi]\n",
    "    bits_for_number_of_segments_array = sampling_record_bits_for_number_of_segments_array[pulse_descriptor_aoi]\n",
    "    bits_for_number_of_samples_array = sampling_record_bits_for_number_of_samples_array[pulse_descriptor_aoi]\n",
    "    number_of_segments_array = sampling_record_number_of_segments_array[pulse_descriptor_aoi]\n",
    "    number_of_samples_array = sampling_record_number_of_samples_array[pulse_descriptor_aoi]\n",
    "    bits_per_sample_array = sampling_record_bits_per_sample_array[pulse_descriptor_aoi]    \n",
    "\n",
    "    wvs_file_offset = offset_to_waves\n",
    "    readbin_wvs_file.seek(wvs_file_offset[0],0)\n",
    "\n",
    "    if(number_of_extra_wave_bytes > 0):\n",
    "        extra_wave_bytes = np.fromfile(readbin_wvs_file,np.int8,number_of_extra_wave_bytes)\n",
    "        \n",
    "    for iSampling in range(number_of_samplings):\n",
    "                \n",
    "        if bits_for_number_of_segments_array[iSampling] != 0:\n",
    "            number_of_segments_number_of_bits = bits_for_number_of_segments_array[iSampling]\n",
    "            if number_of_segments_number_of_bits[0] == 8:\n",
    "                number_of_segments = np.fromfile(readbin_wvs_file,np.uint8,1)\n",
    "            elif number_of_segments_number_of_bits[0] == 16:\n",
    "                number_of_segments = np.fromfile(readbin_wvs_file,np.uint16,1)            \n",
    "            elif number_of_segments_number_of_bits[0] == 32:\n",
    "                number_of_segments = np.fromfile(readbin_wvs_file,np.uint32,1)\n",
    "        else:\n",
    "            number_of_segments = number_of_segments_array[iSampling]    \n",
    "\n",
    "        waveform_segments_array = np.zeros((500,number_of_segments))\n",
    "        number_of_samples_array = np.zeros(number_of_segments)\n",
    "\n",
    "        # Scaled duration is time from reference point (anchor) to current sample point\n",
    "        scaled_duration_from_anchor_for_segment_array = np.zeros(number_of_segments)                    \n",
    "\n",
    "        for iSegment in range(number_of_segments):\n",
    "            duration_from_anchor_number_of_bits = bits_for_duration_from_anchor_array[iSampling]\n",
    "            if duration_from_anchor_number_of_bits == 8:\n",
    "                duration_from_anchor = np.fromfile(readbin_wvs_file,np.uint8,1)\n",
    "            elif duration_from_anchor_number_of_bits == 16:\n",
    "                duration_from_anchor = np.fromfile(readbin_wvs_file,np.uint16,1)            \n",
    "            elif duration_from_anchor_number_of_bits == 32:\n",
    "                duration_from_anchor = np.fromfile(readbin_wvs_file,np.uint32,1)\n",
    "\n",
    "            if bits_for_number_of_samples_array[iSampling] != 0:\n",
    "                number_of_samples_number_of_bits = bits_for_number_of_samples_array[iSampling]    \n",
    "                if number_of_samples_number_of_bits == 8:\n",
    "                    number_of_samples = np.fromfile(readbin_wvs_file,np.uint8,1)\n",
    "                elif number_of_samples_number_of_bits == 16:\n",
    "                    number_of_samples = np.fromfile(readbin_wvs_file,np.uint16,1)            \n",
    "                elif number_of_samples_number_of_bits == 32:\n",
    "                    number_of_samples = np.fromfile(readbin_wvs_file,np.uint32,1)\n",
    "            else:\n",
    "                number_of_samples = number_of_samples_array[iSampling] \n",
    "                        \n",
    "            if number_of_samples > 0:   \n",
    "                bits_per_sample_number_of_bits = bits_per_sample_array[iSampling]\n",
    "                    \n",
    "            if bits_per_sample_number_of_bits == 8:\n",
    "                waveform_segment = np.fromfile(readbin_wvs_file,np.uint8,number_of_samples[0])\n",
    "            elif bits_per_sample_number_of_bits == 16:\n",
    "                waveform_segment = np.fromfile(readbin_wvs_file,np.uint16,number_of_samples[0])            \n",
    "            elif bits_per_sample_number_of_bits == 32:\n",
    "                waveform_segment = np.fromfile(readbin_wvs_file,np.uint32,number_of_samples[0])\n",
    "\n",
    "            scaled_duration_from_anchor_for_segment = float(duration_from_anchor) * scale_for_duration_from_anchor_array[iSampling] + offset_for_duration_from_anchor_array[iSampling]\n",
    "\n",
    "            waveform_segments_array[0:number_of_samples[0],iSegment] = waveform_segment\n",
    "            number_of_samples_array[iSegment] = number_of_samples[0]\n",
    "            scaled_duration_from_anchor_for_segment_array[iSegment] = scaled_duration_from_anchor_for_segment\n",
    "                                                                \n",
    "            waveform_segment = 0\n",
    "\n",
    "        number_of_samples_array = np.int16(number_of_samples_array)\n",
    "        \n",
    "        if iSampling == 0:\n",
    "            neon_waveform_outgoing_pulse = waveform_segments_array[0:number_of_samples_array[0]]\n",
    "        else:\n",
    "            scaled_duration_from_anchor_for_segment0 = scaled_duration_from_anchor_for_segment_array[0]\n",
    "            good_segments_aoi = np.argwhere(scaled_duration_from_anchor_for_segment_array > 0)\n",
    "            good_segments_aoi = np.reshape(good_segments_aoi,len(good_segments_aoi))\n",
    "            number_of_good_segments = len(good_segments_aoi)\n",
    "            if number_of_good_segments > 1:\n",
    "                # Combine multiple segments\n",
    "                neon_waveform_return_pulse = np.zeros(np.int16(scaled_duration_from_anchor_for_segment_array[np.max(good_segments_aoi)] - scaled_duration_from_anchor_for_segment_array[0] + number_of_samples_array[np.max(good_segments_aoi)]))\n",
    "                for iSegment in range(number_of_good_segments):\n",
    "                    array_start_pos = np.int16(scaled_duration_from_anchor_for_segment_array[good_segments_aoi[iSegment]] - scaled_duration_from_anchor_for_segment_array[0])\n",
    "                    neon_waveform_return_pulse[array_start_pos:array_start_pos+number_of_samples_array[good_segments_aoi[iSegment]]] = waveform_segments_array[0:number_of_samples_array[good_segments_aoi[iSegment]],good_segments_aoi[iSegment]]                                                     \n",
    "            else:\n",
    "                neon_waveform_return_pulse = waveform_segments_array[0:number_of_samples_array[0]-1,0]\n",
    "   \n",
    "        waveform_segments_array = 0\n",
    "        number_of_samples_array = 0\n",
    "        scaled_duration_from_anchor_for_segment_array = 0    \n",
    "        # TODO: Remove break\n",
    "        break\n",
    "    \n",
    "    # Determine peak location of the outgoing pulse\n",
    "    outgoing_peak_pos = np.argwhere(neon_waveform_outgoing_pulse == np.max(neon_waveform_outgoing_pulse))\n",
    "    if len(outgoing_peak_pos) == 1:\n",
    "        neon_waveform_outgoing_pulse_peak_location = outgoing_peak_pos[0,0]  \n",
    "    else:\n",
    "        neon_waveform_outgoing_pulse_peak_location = np.mean(outgoing_peak_pos[:,0])\n",
    "\n",
    "    # Adjust the duration from anchor from bin0 to the peak location of the outgoing pulse\n",
    "    scaled_duration_from_anchor_for_segment0_adj = scaled_duration_from_anchor_for_segment0 - neon_waveform_outgoing_pulse_peak_location\n",
    "            \n",
    "    # Fill in multiple segment gaps with the minimum valid return waveform value, then subtract the min for normalization\n",
    "    return_good_aoi = np.argwhere(neon_waveform_return_pulse > 0)\n",
    "    return_good_aoi = np.reshape(return_good_aoi,len(return_good_aoi))       \n",
    "    return_bad_aoi = np.argwhere(neon_waveform_return_pulse == 0)\n",
    "    return_bad_aoi = np.reshape(return_bad_aoi,len(return_bad_aoi))                          \n",
    "    if len(return_bad_aoi) > 0:\n",
    "        neon_waveform_return_pulse[return_bad_aoi] = np.min(neon_waveform_return_pulse[return_good_aoi])\n",
    "    neon_waveform_offset = np.min(neon_waveform_return_pulse[return_good_aoi])\n",
    "    neon_waveform_return_pulse = neon_waveform_return_pulse - neon_waveform_offset\n",
    "        \n",
    "    # Calculate absolute x,y,z axes for pulse\n",
    "    neon_waveform_x_axis = (np.arange(len(neon_waveform_return_pulse)) + scaled_duration_from_anchor_for_segment0_adj) * dx + x_anchor\n",
    "    neon_waveform_y_axis = (np.arange(len(neon_waveform_return_pulse)) + scaled_duration_from_anchor_for_segment0_adj) * dy + y_anchor\n",
    "    neon_waveform_z_axis = (np.arange(len(neon_waveform_return_pulse)) + scaled_duration_from_anchor_for_segment0_adj) * dz + z_anchor\n",
    "    \n",
    "    # Return neon_waveform_return_pulse, neon_waveform_x_axis, neon_waveform_y_axis, neon_waveform_z_axis\n",
    "    return neon_waveform_return_pulse, neon_waveform_x_axis, neon_waveform_y_axis, neon_waveform_z_axis, neon_waveform_offset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8d6408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_LVIS1B_H5_pulse_information(H5_file,ellipsoid_to_geoid_offset):\n",
    "    '''A function to read pulse information from a LVIS H5 file'''\n",
    "\n",
    "    LVIS1B = h5py.File(H5_file, 'r')\n",
    "\n",
    "    a_text = str(LVIS1B[f'ancillary_data/ancillary_text'][()])\n",
    "    a_text2 = a_text.split('\\\\n')\n",
    "    for iLine in range(len(a_text2)):\n",
    "        a_text3 = a_text2[iLine].split(' ')\n",
    "        if (a_text3[0] == 'Instrument:'):\n",
    "            instrument_name = a_text3[1]\n",
    "        a_text3 = 0\n",
    "    a_text = 0\n",
    "    a_text2 = 0\n",
    "    \n",
    "    # LVIS Read in Shot Number and First/Last Coordinates for every waveform\n",
    "    LVIS1B_shot_number_array,LVIS1B_range_array,LVIS1B_latitude_bin0_array,LVIS1B_longitude_bin0_array,LVIS1B_elevation_bin0_array,LVIS1B_latitude_lastbin_array,LVIS1B_longitude_lastbin_array,LVIS1B_elevation_lastbin_array = [],[],[],[],[],[],[],[]\n",
    "    LVIS1B_shots = LVIS1B[f'SHOTNUMBER'][()]\n",
    "    LVIS1B_ranges = LVIS1B[f'RANGE'][()]\n",
    "    LVIS1B_lat0s = LVIS1B[f'LAT0'][()]\n",
    "    LVIS1B_lon0s = LVIS1B[f'LON0'][()]\n",
    "    LVIS1B_z0s = LVIS1B[f'Z0'][()]\n",
    "    if instrument_name == 'LVIS-C':\n",
    "        LVIS1B_wfCount = 1024\n",
    "        LVIS1B_latNs = LVIS1B[f'LAT1023'][()]\n",
    "        LVIS1B_lonNs = LVIS1B[f'LON1023'][()]\n",
    "        LVIS1B_zNs = LVIS1B[f'Z1023'][()]\n",
    "    if instrument_name == 'LVIS-F':\n",
    "        LVIS1B_wfCount = 1216\n",
    "        LVIS1B_latNs = LVIS1B[f'LAT1215'][()]\n",
    "        LVIS1B_lonNs = LVIS1B[f'LON1215'][()]\n",
    "        LVIS1B_zNs = LVIS1B[f'Z1215'][()]\n",
    "\n",
    "    for iShot in range(len(LVIS1B_shots)):\n",
    "        LVIS1B_shot_number_array.append(LVIS1B_shots[iShot])\n",
    "        LVIS1B_range_array.append(LVIS1B_ranges[iShot])\n",
    "        LVIS1B_latitude_bin0_array.append(LVIS1B_lat0s[iShot])\n",
    "        LVIS1B_longitude_bin0_array.append(LVIS1B_lon0s[iShot]-360.0)\n",
    "        LVIS1B_elevation_bin0_array.append(LVIS1B_z0s[iShot])    \n",
    "        LVIS1B_latitude_lastbin_array.append(LVIS1B_latNs[iShot])\n",
    "        LVIS1B_longitude_lastbin_array.append(LVIS1B_lonNs[iShot]-360.0)\n",
    "        LVIS1B_elevation_lastbin_array.append(LVIS1B_zNs[iShot]) \n",
    "    \n",
    "    LVIS1B_num_shots = len(LVIS1B_shots)\n",
    "    \n",
    "    platform_location_array = np.zeros((LVIS1B_num_shots,3))\n",
    "    dxdydz_array = np.zeros((LVIS1B_num_shots,3))\n",
    "    xyz_first_array = np.zeros((LVIS1B_num_shots,3))\n",
    "    xyz_last_array = np.zeros((LVIS1B_num_shots,3))\n",
    "        \n",
    "    for iShot in range(LVIS1B_num_shots):\n",
    "        x_bin0, y_bin0 = pp(LVIS1B_longitude_bin0_array[iShot], LVIS1B_latitude_bin0_array[iShot])\n",
    "        x_lastbin, y_lastbin = pp(LVIS1B_longitude_lastbin_array[iShot], LVIS1B_latitude_lastbin_array[iShot])\n",
    "        zStart = LVIS1B_elevation_bin0_array[iShot] + ellipsoid_to_geoid_offset \n",
    "        zEnd = LVIS1B_elevation_lastbin_array[iShot] + ellipsoid_to_geoid_offset \n",
    "\n",
    "        dxdydz_array[iShot,0] = (x_lastbin - x_bin0) / (LVIS1B_wfCount-1)\n",
    "        dxdydz_array[iShot,1] = (y_lastbin - y_bin0) / (LVIS1B_wfCount-1)\n",
    "        dxdydz_array[iShot,2] = (zEnd - zStart) / (LVIS1B_wfCount-1)\n",
    "\n",
    "        dr = np.sqrt(dxdydz_array[iShot,0] ** 2 + dxdydz_array[iShot,1] ** 2 + dxdydz_array[iShot,2] ** 2)\n",
    "        range_num_bins = LVIS1B_range_array[iShot] / dr\n",
    "\n",
    "        xyz_first_array[iShot,0] = x_bin0\n",
    "        xyz_first_array[iShot,1] = y_bin0\n",
    "        xyz_first_array[iShot,2] = zStart\n",
    "        \n",
    "        xyz_last_array[iShot,0] = x_lastbin\n",
    "        xyz_last_array[iShot,1] = y_lastbin   \n",
    "        xyz_last_array[iShot,2] = zEnd   \n",
    "\n",
    "        # For now this is defined based on the first bin, need to adjust for actual ground bin location\n",
    "        platform_location_array[iShot,0] = xyz_first_array[iShot,0] + dxdydz_array[iShot,0] * -1.0 * range_num_bins \n",
    "        platform_location_array[iShot,1] = xyz_first_array[iShot,1] + dxdydz_array[iShot,1] * -1.0 * range_num_bins\n",
    "        platform_location_array[iShot,2] = xyz_first_array[iShot,2] + dxdydz_array[iShot,2] * -1.0 * range_num_bins\n",
    "\n",
    "    LVIS1B_shot_number_array = np.int64(LVIS1B_shot_number_array)\n",
    "    LVIS1B_shot_number_array = np.reshape(LVIS1B_shot_number_array,len(LVIS1B_shot_number_array))\n",
    "        \n",
    "    LVIS1B.close()\n",
    "\n",
    "    LVIS1B_shots = 0\n",
    "    LVIS1B_ranges = 0\n",
    "    LVIS1B_lat0s = 0\n",
    "    LVIS1B_lon0s = 0\n",
    "    LVIS1B_z0s = 0\n",
    "    LVIS1B_latNs = 0\n",
    "    LVIS1B_lonNs = 0\n",
    "    LVIS1B_zNs = 0\n",
    "    LVIS1B_latitude_bin0_array = 0\n",
    "    LVIS1B_longitude_bin0_array = 0\n",
    "    LVIS1B_elevation_bin0_array = 0\n",
    "    LVIS1B_latitude_lastbin_array = 0\n",
    "    LVIS1B_longitude_lastbin_array = 0\n",
    "    LVIS1B_elevation_lastbin_array = 0\n",
    "        \n",
    "    return instrument_name, LVIS1B_num_shots, platform_location_array, dxdydz_array, xyz_first_array, xyz_last_array, LVIS1B_wfCount, LVIS1B_shot_number_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e144d58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_LVIS1B_H5_waveform(readbin_waveform_h5_file,iPulse,pulse_size,xyz_bin0,xyz_lastbin):\n",
    "    '''A function to read a waveform from a LVIS H5 file'''\n",
    "    \n",
    "    LVIS1B_wfCount = pulse_size\n",
    "        \n",
    "    waveform = readbin_waveform_h5_file[f'RXWAVE'][iPulse]\n",
    "    offset = np.mean(waveform[0:100])\n",
    "    stdev = np.std(waveform[0:100])\n",
    "    eastingStart = xyz_bin0[0]\n",
    "    eastingEnd = xyz_lastbin[0]    \n",
    "    waveform_x_axis = np.arange(LVIS1B_wfCount) * (eastingEnd - eastingStart) / (LVIS1B_wfCount-1) + eastingStart\n",
    "    northingStart = xyz_bin0[1]\n",
    "    northingEnd = xyz_lastbin[1]    \n",
    "    waveform_y_axis = np.arange(LVIS1B_wfCount) * (northingEnd - northingStart) / (LVIS1B_wfCount-1) + northingStart\n",
    "    zStart = xyz_bin0[2]\n",
    "    zEnd = xyz_lastbin[2]\n",
    "    waveform_z_axis = np.arange(LVIS1B_wfCount) * (zEnd - zStart) / (LVIS1B_wfCount-1) + zStart\n",
    "    waveform_return_pulse = waveform - offset\n",
    "    \n",
    "    waveform = 0\n",
    "    \n",
    "    # Return waveform_return_pulse, waveform_x_axis, waveform_y_axis, waveform_z_axis\n",
    "    return waveform_return_pulse, waveform_x_axis, waveform_y_axis, waveform_z_axis, offset, stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "567085de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_GEDIL1B_H5_pulse_information(H5_file,ellipsoid_to_geoid_offset):\n",
    "    '''A function to read pulse information from a GEDI H5 file'''\n",
    "\n",
    "    GEDIL1B = h5py.File(H5_file, 'r')\n",
    "\n",
    "    beams = np.array(('BEAM0000','BEAM0001','BEAM0010','BEAM0011','BEAM0101','BEAM0110','BEAM1000','BEAM1011'))\n",
    "\n",
    "    for iBeam in range(len(beams)):\n",
    "        GEDIL1B_BEAM_shot_number_array = GEDIL1B[f'{beams[iBeam]}/geolocation/shot_number'][()]\n",
    "        GEDIL1B_BEAM_altitude_instrument_array = GEDIL1B[f'{beams[iBeam]}/geolocation/altitude_instrument'][()]\n",
    "        GEDIL1B_BEAM_elevation_bin0_array = GEDIL1B[f'{beams[iBeam]}/geolocation/elevation_bin0'][()]\n",
    "        GEDIL1B_BEAM_elevation_lastbin_array = GEDIL1B[f'{beams[iBeam]}/geolocation/elevation_lastbin'][()]\n",
    "        GEDIL1B_BEAM_latitude_bin0_array = GEDIL1B[f'{beams[iBeam]}/geolocation/latitude_bin0'][()]\n",
    "        GEDIL1B_BEAM_latitude_lastbin_array = GEDIL1B[f'{beams[iBeam]}/geolocation/latitude_lastbin'][()]\n",
    "        GEDIL1B_BEAM_longitude_bin0_array = GEDIL1B[f'{beams[iBeam]}/geolocation/longitude_bin0'][()]\n",
    "        GEDIL1B_BEAM_longitude_lastbin_array = GEDIL1B[f'{beams[iBeam]}/geolocation/longitude_lastbin'][()]\n",
    "        GEDIL1B_BEAM_rx_sample_count_array = GEDIL1B[f'{beams[iBeam]}/rx_sample_count'][()]\n",
    "        GEDIL1B_BEAM_num_shots = np.size(GEDIL1B_BEAM_shot_number_array)\n",
    "        GEDIL1B_BEAM_beam_name_array = np.chararray(GEDIL1B_BEAM_num_shots,itemsize=8)\n",
    "        GEDIL1B_BEAM_beam_name_array[:] = str(beams[iBeam])\n",
    "        GEDIL1B_BEAM_iPulse_pos_array = np.arange(GEDIL1B_BEAM_num_shots)\n",
    "        \n",
    "        # Combine data from beams\n",
    "        if iBeam == 0:\n",
    "            GEDIL1B_shot_number_array = GEDIL1B_BEAM_shot_number_array\n",
    "            GEDIL1B_altitude_instrument_array = GEDIL1B_BEAM_altitude_instrument_array\n",
    "            GEDIL1B_elevation_bin0_array = GEDIL1B_BEAM_elevation_bin0_array\n",
    "            GEDIL1B_elevation_lastbin_array = GEDIL1B_BEAM_elevation_lastbin_array\n",
    "            GEDIL1B_latitude_bin0_array = GEDIL1B_BEAM_latitude_bin0_array\n",
    "            GEDIL1B_latitude_lastbin_array = GEDIL1B_BEAM_latitude_lastbin_array\n",
    "            GEDIL1B_longitude_bin0_array = GEDIL1B_BEAM_longitude_bin0_array\n",
    "            GEDIL1B_longitude_lastbin_array = GEDIL1B_BEAM_longitude_lastbin_array\n",
    "            GEDIL1B_rx_sample_count_array = GEDIL1B_BEAM_rx_sample_count_array\n",
    "            GEDIL1B_beam_name_array = GEDIL1B_BEAM_beam_name_array.decode()\n",
    "            GEDIL1B_iPulse_pos_array = GEDIL1B_BEAM_iPulse_pos_array            \n",
    "        else:\n",
    "            GEDIL1B_shot_number_array = np.concatenate((GEDIL1B_shot_number_array,GEDIL1B_BEAM_shot_number_array),axis=0)\n",
    "            GEDIL1B_altitude_instrument_array = np.concatenate((GEDIL1B_altitude_instrument_array,GEDIL1B_BEAM_altitude_instrument_array),axis=0)\n",
    "            GEDIL1B_elevation_bin0_array = np.concatenate((GEDIL1B_elevation_bin0_array,GEDIL1B_BEAM_elevation_bin0_array),axis=0)\n",
    "            GEDIL1B_elevation_lastbin_array = np.concatenate((GEDIL1B_elevation_lastbin_array,GEDIL1B_BEAM_elevation_lastbin_array),axis=0)\n",
    "            GEDIL1B_latitude_bin0_array = np.concatenate((GEDIL1B_latitude_bin0_array,GEDIL1B_BEAM_latitude_bin0_array),axis=0)\n",
    "            GEDIL1B_latitude_lastbin_array = np.concatenate((GEDIL1B_latitude_lastbin_array,GEDIL1B_BEAM_latitude_lastbin_array),axis=0)\n",
    "            GEDIL1B_longitude_bin0_array = np.concatenate((GEDIL1B_longitude_bin0_array,GEDIL1B_BEAM_longitude_bin0_array),axis=0)\n",
    "            GEDIL1B_longitude_lastbin_array = np.concatenate((GEDIL1B_longitude_lastbin_array,GEDIL1B_BEAM_longitude_lastbin_array),axis=0)\n",
    "            GEDIL1B_rx_sample_count_array = np.concatenate((GEDIL1B_rx_sample_count_array,GEDIL1B_BEAM_rx_sample_count_array),axis=0)\n",
    "            GEDIL1B_beam_name_array = np.concatenate((GEDIL1B_beam_name_array,GEDIL1B_BEAM_beam_name_array),axis=0)\n",
    "            GEDIL1B_iPulse_pos_array = np.concatenate((GEDIL1B_iPulse_pos_array,GEDIL1B_BEAM_iPulse_pos_array),axis=0)\n",
    "        \n",
    "        GEDIL1B_BEAM_shot_number_array = 0\n",
    "        GEDIL1B_BEAM_elevation_bin0_array = 0\n",
    "        GEDIL1B_BEAM_elevation_lastbin_array = 0\n",
    "        GEDIL1B_BEAM_latitude_bin0_array = 0\n",
    "        GEDIL1B_BEAM_latitude_lastbin_array = 0\n",
    "        GEDIL1B_BEAM_longitude_bin0_array = 0\n",
    "        GEDIL1B_BEAM_longitude_lastbin_array = 0\n",
    "        GEDIL1B_BEAM_rx_sample_count_array = 0\n",
    "        GEDIL1B_BEAM_beam_name_array = 0\n",
    "        GEDIL1B_BEAM_iPulse_pos_array = 0             \n",
    "    \n",
    "    GEDIL1B_num_shots = np.size(GEDIL1B_shot_number_array)\n",
    "    \n",
    "    platform_location_array = np.zeros((GEDIL1B_num_shots,3))\n",
    "    dxdydz_array = np.zeros((GEDIL1B_num_shots,3))\n",
    "    xyz_first_array = np.zeros((GEDIL1B_num_shots,3))\n",
    "    xyz_last_array = np.zeros((GEDIL1B_num_shots,3))\n",
    "        \n",
    "    for iShot in range(GEDIL1B_num_shots):\n",
    "        GEDIL1B_wfCount = GEDIL1B_rx_sample_count_array[iShot]\n",
    "\n",
    "        x_bin0, y_bin0 = pp(GEDIL1B_longitude_bin0_array[iShot], GEDIL1B_latitude_bin0_array[iShot])\n",
    "        x_lastbin, y_lastbin = pp(GEDIL1B_longitude_lastbin_array[iShot], GEDIL1B_latitude_lastbin_array[iShot])\n",
    "        zStart = GEDIL1B_elevation_bin0_array[iShot] + ellipsoid_to_geoid_offset \n",
    "        zEnd = GEDIL1B_elevation_lastbin_array[iShot] + ellipsoid_to_geoid_offset \n",
    "\n",
    "        dxdydz_array[iShot,0] = (x_lastbin - x_bin0) / (GEDIL1B_wfCount-1)\n",
    "        dxdydz_array[iShot,1] = (y_lastbin - y_bin0) / (GEDIL1B_wfCount-1)\n",
    "        dxdydz_array[iShot,2] = (zEnd - zStart) / (GEDIL1B_wfCount-1)\n",
    "\n",
    "        elev_diff = GEDIL1B_altitude_instrument_array[iShot] - GEDIL1B_elevation_bin0_array[iShot] \n",
    "        range_num_bins = elev_diff / dxdydz_array[iShot,2] * -1.0\n",
    "        \n",
    "        xyz_first_array[iShot,0] = x_bin0\n",
    "        xyz_first_array[iShot,1] = y_bin0\n",
    "        xyz_first_array[iShot,2] = zStart\n",
    "        \n",
    "        xyz_last_array[iShot,0] = x_lastbin\n",
    "        xyz_last_array[iShot,1] = y_lastbin   \n",
    "        xyz_last_array[iShot,2] = zEnd   \n",
    "\n",
    "        #calculating using ISS altitude\n",
    "        platform_location_array[iShot,0] = xyz_first_array[iShot,0] + dxdydz_array[iShot,0] * -1.0 * range_num_bins\n",
    "        platform_location_array[iShot,1] = xyz_first_array[iShot,1] + dxdydz_array[iShot,1] * -1.0 * range_num_bins\n",
    "        platform_location_array[iShot,2] = xyz_first_array[iShot,2] + dxdydz_array[iShot,2] * -1.0 * range_num_bins\n",
    "        \n",
    "    GEDIL1B_shot_number_array = np.int64(GEDIL1B_shot_number_array)\n",
    "    GEDIL1B_shot_number_array = np.reshape(GEDIL1B_shot_number_array,len(GEDIL1B_shot_number_array))\n",
    "    \n",
    "    GEDIL1B.close()\n",
    "  \n",
    "    GEDIL1B_altitude_instrument_array = 0\n",
    "    GEDIL1B_elevation_bin0_array = 0\n",
    "    GEDIL1B_elevation_lastbin_array = 0\n",
    "    GEDIL1B_latitude_bin0_array = 0\n",
    "    GEDIL1B_latitude_lastbin_array = 0\n",
    "    GEDIL1B_longitude_bin0_array = 0\n",
    "    GEDIL1B_longitude_lastbin_array = 0\n",
    "    GEDIL1B_rx_sample_count_array = 0\n",
    "\n",
    "    return GEDIL1B_num_shots, platform_location_array, dxdydz_array, xyz_first_array, xyz_last_array, GEDIL1B_wfCount, GEDIL1B_shot_number_array, GEDIL1B_beam_name_array, GEDIL1B_iPulse_pos_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64edc0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_GEDIL1B_H5_waveform(readbin_waveform_h5_file,iPulse,xyz_bin0,xyz_lastbin,beam_name,iPulse_pos):\n",
    "    '''A function to read a waveform from a GEDI H5 file'''\n",
    "    \n",
    "    rx_sample_count = readbin_waveform_h5_file[f'{beam_name}/rx_sample_count'][iPulse_pos]\n",
    "    rx_sample_start_index = int(readbin_waveform_h5_file[f'{beam_name}/rx_sample_start_index'][iPulse_pos]-1)\n",
    "    waveform = readbin_waveform_h5_file[f'{beam_name}/rxwaveform'][rx_sample_start_index:rx_sample_start_index+rx_sample_count]\n",
    "    offset = np.mean(waveform[0:100])\n",
    "    stdev = np.std(waveform[0:100])\n",
    "    \n",
    "    eastingStart = xyz_bin0[0]\n",
    "    eastingEnd = xyz_lastbin[0]    \n",
    "    waveform_x_axis = np.arange(rx_sample_count) * (eastingEnd - eastingStart) / (rx_sample_count-1) + eastingStart\n",
    "    northingStart = xyz_bin0[1]\n",
    "    northingEnd = xyz_lastbin[1]    \n",
    "    waveform_y_axis = np.arange(rx_sample_count) * (northingEnd - northingStart) / (rx_sample_count-1) + northingStart\n",
    "    zStart = xyz_bin0[2]\n",
    "    zEnd = xyz_lastbin[2]\n",
    "    waveform_z_axis = np.arange(rx_sample_count) * (zEnd - zStart) / (rx_sample_count-1) + zStart\n",
    "    \n",
    "    waveform_return_pulse = waveform - offset    \n",
    "    \n",
    "    waveform = 0\n",
    "    \n",
    "    # Return waveform_return_pulse, waveform_x_axis, waveform_y_axis, waveform_z_axis\n",
    "    return waveform_return_pulse, waveform_x_axis, waveform_y_axis, waveform_z_axis, offset, stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a1a6aefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_level1_lidar_pulse_information(lidar_sensor_name,lidar_instrument_name,waveform_file,air_index_of_refraction,c,ellipsoid_to_geoid_offset):\n",
    "    '''A function to get waveform information for multiple different AOP and non-aop sensors'''\n",
    "\n",
    "    if lidar_sensor_name == 'AOP':\n",
    "        waveform_files = waveform_file.split('.')\n",
    "        pls_file = waveform_files[0] + '.pls'\n",
    "        instrument_name, number_of_pulses, xyz_origin_array, dxdydz_array, xyz_bin0_array, xyz_lastbin_array, offset_to_pulse_data, pulse_size, T_scale_factor, T_offset, x_scale_factor, x_offset, y_scale_factor, y_offset, z_scale_factor, z_offset, sampling_record_pulse_descriptor_index_lookup_array, pulse_descriptor_optical_center_to_anchor_point_array, pulse_descriptor_number_of_extra_wave_bytes_array, pulse_descriptor_number_of_samplings_array, sampling_record_bits_for_duration_from_anchor_array, sampling_record_scale_for_duration_from_anchor_array, sampling_record_offset_for_duration_from_anchor_array, sampling_record_bits_for_number_of_segments_array, sampling_record_bits_for_number_of_samples_array, sampling_record_number_of_segments_array, sampling_record_number_of_samples_array, sampling_record_bits_per_sample_array = read_NEONAOP_pulsewaves_pulse_information(pls_file,lidar_instrument_name) \n",
    "        print(instrument_name)\n",
    "        if (instrument_name == 'Gemini'):\n",
    "            beam_footprint = 1.13137 # 0.8 1/e\n",
    "        elif (instrument_name == 'Galaxy'):\n",
    "            beam_footprint = 0.533553 # 0.25 1/e\n",
    "        elif (instrument_name == 'LMS-Q780'):\n",
    "            beam_footprint = 0.25 # 0.25 1/e^2\n",
    "            xyz_origin_array = xyz_origin_array + dxdydz_array * -1000.0\n",
    "        shot_number_array = np.arange(number_of_pulses)\n",
    "        beam_name_array = np.chararray(number_of_pulses,itemsize=8)\n",
    "        beam_name_array[:] = 'NA'\n",
    "        iPulse_pos_array = np.arange(number_of_pulses)\n",
    "    else:\n",
    "        offset_to_pulse_data = 0\n",
    "        #pulse_size = 0\n",
    "        T_scale_factor = 0\n",
    "        T_offset = 0\n",
    "        x_scale_factor = 0\n",
    "        x_offset = 0\n",
    "        y_scale_factor = 0\n",
    "        y_offset = 0\n",
    "        z_scale_factor = 0\n",
    "        z_offset = 0\n",
    "        sampling_record_pulse_descriptor_index_lookup_array = 0\n",
    "        pulse_descriptor_optical_center_to_anchor_point_array = 0\n",
    "        pulse_descriptor_number_of_extra_wave_bytes_array = 0\n",
    "        pulse_descriptor_number_of_samplings_array = 0\n",
    "        sampling_record_bits_for_duration_from_anchor_array = 0\n",
    "        sampling_record_scale_for_duration_from_anchor_array = 0\n",
    "        sampling_record_offset_for_duration_from_anchor_array = 0\n",
    "        sampling_record_bits_for_number_of_segments_array = 0\n",
    "        sampling_record_bits_for_number_of_samples_array = 0\n",
    "        sampling_record_number_of_segments_array = 0\n",
    "        sampling_record_number_of_samples_array = 0\n",
    "        sampling_record_bits_per_sample_array = 0       \n",
    "        if (lidar_sensor_name == 'LVIS'):\n",
    "            instrument_name, number_of_pulses, xyz_origin_array, dxdydz_array, xyz_bin0_array, xyz_lastbin_array, pulse_size, shot_number_array = read_LVIS1B_H5_pulse_information(waveform_file,ellipsoid_to_geoid_offset)\n",
    "            if (instrument_name == 'LVIS-C'):\n",
    "                beam_footprint = 25.0\n",
    "            if (instrument_name == 'LVIS-F'):\n",
    "                beam_footprint = 10.0\n",
    "            beam_name_array = np.chararray(number_of_pulses,itemsize=8)\n",
    "            beam_name_array[:] = 'NA'\n",
    "            iPulse_pos_array = np.arange(number_of_pulses)\n",
    "        else:\n",
    "            if lidar_sensor_name == 'GEDI': \n",
    "                number_of_pulses, xyz_origin_array, dxdydz_array, xyz_bin0_array, xyz_lastbin_array, pulse_size, shot_number_array, beam_name_array, iPulse_pos_array = read_GEDIL1B_H5_pulse_information(waveform_file,ellipsoid_to_geoid_offset)\n",
    "                instrument_name = 'GEDI'\n",
    "                beam_footprint = 23.0\n",
    "            else:\n",
    "                print('Error: Lidar Sensor Name = ', lidar_sensor_name, ' is not valid')\n",
    "        \n",
    "        \n",
    "    return instrument_name, beam_footprint, number_of_pulses, xyz_origin_array, dxdydz_array, xyz_bin0_array, xyz_lastbin_array, shot_number_array, beam_name_array, iPulse_pos_array, offset_to_pulse_data, pulse_size, T_scale_factor, T_offset, x_scale_factor, x_offset, y_scale_factor, y_offset, z_scale_factor, z_offset, sampling_record_pulse_descriptor_index_lookup_array, pulse_descriptor_optical_center_to_anchor_point_array, pulse_descriptor_number_of_extra_wave_bytes_array, pulse_descriptor_number_of_samplings_array, sampling_record_bits_for_duration_from_anchor_array, sampling_record_scale_for_duration_from_anchor_array, sampling_record_offset_for_duration_from_anchor_array, sampling_record_bits_for_number_of_segments_array, sampling_record_bits_for_number_of_samples_array, sampling_record_number_of_segments_array, sampling_record_number_of_samples_array, sampling_record_bits_per_sample_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "45636f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to read in a waveform\n",
    "def get_level1_lidar_waveform(lidar_sensor_name,instrument_name,lidar_instrument_name,waveform_file,iPulse,offset_to_pulse_data,pulse_size,T_scale_factor,T_offset,x_scale_factor,x_offset,y_scale_factor,y_offset,z_scale_factor,z_offset,sampling_record_pulse_descriptor_index_lookup_array,pulse_descriptor_optical_center_to_anchor_point_array,pulse_descriptor_number_of_extra_wave_bytes_array,pulse_descriptor_number_of_samplings_array,sampling_record_bits_for_duration_from_anchor_array,sampling_record_scale_for_duration_from_anchor_array,sampling_record_offset_for_duration_from_anchor_array,sampling_record_bits_for_number_of_segments_array,sampling_record_bits_for_number_of_samples_array,sampling_record_number_of_segments_array,sampling_record_number_of_samples_array,sampling_record_bits_per_sample_array,dxdydz,xyz_bin0,xyz_lastbin,beam_name,iPulse_pos):\n",
    "    if lidar_sensor_name == 'AOP':\n",
    "        waveform_files = waveform_file.split('.')\n",
    "        pls_file = waveform_files[0] + '.pls'\n",
    "        wvs_file = waveform_files[0] + '.wvs'\n",
    "        readbin_pls_file = open(pls_file,\"rb\")\n",
    "        readbin_pls_file.seek(0,0)\n",
    "        readbin_wvs_file = open(wvs_file,\"rb\")\n",
    "        readbin_wvs_file.seek(0,0)    \n",
    "        waveform, waveform_x_axis, waveform_y_axis, waveform_z_axis, offset = read_NEONAOP_pulsewaves_waveform(readbin_pls_file,readbin_wvs_file,instrument_name,lidar_instrument_name,iPulse,offset_to_pulse_data,pulse_size,T_scale_factor,T_offset,x_scale_factor,x_offset,y_scale_factor,y_offset,z_scale_factor,z_offset,sampling_record_pulse_descriptor_index_lookup_array,pulse_descriptor_optical_center_to_anchor_point_array,pulse_descriptor_number_of_extra_wave_bytes_array,pulse_descriptor_number_of_samplings_array,sampling_record_bits_for_duration_from_anchor_array,sampling_record_scale_for_duration_from_anchor_array,sampling_record_offset_for_duration_from_anchor_array,sampling_record_bits_for_number_of_segments_array,sampling_record_bits_for_number_of_samples_array,sampling_record_number_of_segments_array,sampling_record_number_of_samples_array,sampling_record_bits_per_sample_array)\n",
    "        readbin_pls_file.close()\n",
    "        readbin_wvs_file.close()\n",
    "        stdev = 0.0\n",
    "    else:\n",
    "        if lidar_sensor_name == 'LVIS':\n",
    "            readbin_waveform_h5_file = h5py.File(waveform_file, 'r')\n",
    "            waveform, waveform_x_axis, waveform_y_axis, waveform_z_axis, offset, stdev = read_LVIS1B_H5_waveform(readbin_waveform_h5_file,iPulse,pulse_size,xyz_bin0,xyz_lastbin)\n",
    "            readbin_waveform_h5_file.close()\n",
    "        else:\n",
    "            if lidar_sensor_name == 'GEDI': \n",
    "                readbin_waveform_h5_file = h5py.File(waveform_file, 'r')\n",
    "                waveform, waveform_x_axis, waveform_y_axis, waveform_z_axis, offset, stdev = read_GEDIL1B_H5_waveform(readbin_waveform_h5_file,iPulse,xyz_bin0,xyz_lastbin,beam_name,iPulse_pos)\n",
    "                readbin_waveform_h5_file.close()   \n",
    "            else:\n",
    "                print('Error: Lidar Name = ', lidar_name, ' is not valid')        \n",
    " \n",
    "        \n",
    "    return waveform, waveform_x_axis, waveform_y_axis, waveform_z_axis, offset, stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "af00a447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/felixyu/Documents/Remote-Sensing/Neon-Pls-Wvs-Las/data/NEON_D01_HARV_DP1_L022-1_2019082013_1.pls\n",
      "instrument_name =  Gemini\n",
      "Gemini\n",
      "\n",
      "Processing Step Complete\n",
      "9.38228988647461\n",
      "\n",
      "28036820\n",
      "0.0 2591.3471302629896 726525.4\n",
      "0.0 16748.885144617332 4695875.48\n",
      "0.0 4.2088138494308565 1181.34\n",
      "-0.02896999999997206 5.3710962940876206e-05 0.04951000000000931\n",
      "-0.0055899999998509885 3.31975880288847e-06 0.005129999999888241\n",
      "-0.1499000000000001 -0.0005274246615700355 0.0\n",
      "0.0 2591.7166637217742 726881.44436\n",
      "0.0 16748.907912584396 4695909.26\n",
      "0.0 0.6011119096587981 206.63203999999894\n",
      "0.0 2591.718984978794 726883.37525\n",
      "0.0 16748.908054493182 4695909.494999999\n",
      "0.0 0.5781679106510648 200.0187500000003\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''Read waveform information for AOP into global variables necessary for future analysis   \n",
    "   Process entire pulsewaves file at a time and read in geolocation information for future location filtering\n",
    "'''\n",
    "\n",
    "start_time1 = time.time()\n",
    "\n",
    "print(waveform_file)\n",
    "    \n",
    "# Read pulse ancillary info from waveform file: origin/platform/anchor location, ray dxdydz, first and last bin geolocation\n",
    "instrument_name, beam_footprint, number_of_pulses, xyz_origin_array, dxdydz_array, xyz_bin0_array, xyz_lastbin_array, shot_number_array, beam_name_array, iPulse_pos_array, offset_to_pulse_data, pulse_size, T_scale_factor, T_offset, x_scale_factor, x_offset, y_scale_factor, y_offset, z_scale_factor, z_offset, sampling_record_pulse_descriptor_index_lookup_array, pulse_descriptor_optical_center_to_anchor_point_array, pulse_descriptor_number_of_extra_wave_bytes_array, pulse_descriptor_number_of_samplings_array, sampling_record_bits_for_duration_from_anchor_array, sampling_record_scale_for_duration_from_anchor_array, sampling_record_offset_for_duration_from_anchor_array, sampling_record_bits_for_number_of_segments_array, sampling_record_bits_for_number_of_samples_array, sampling_record_number_of_segments_array, sampling_record_number_of_samples_array, sampling_record_bits_per_sample_array = get_level1_lidar_pulse_information(lidar_sensor_name,lidar_instrument_name,waveform_file,air_index_of_refraction,c,ellipsoid_to_geoid_offset)\n",
    "\n",
    "end_time1 = time.time()\n",
    "\n",
    "x_origin_array = xyz_origin_array[:,0]\n",
    "y_origin_array = xyz_origin_array[:,1]\n",
    "z_origin_array = xyz_origin_array[:,2]\n",
    "dx_array = dxdydz_array[:,0]\n",
    "dy_array = dxdydz_array[:,1]\n",
    "dz_array = dxdydz_array[:,2]\n",
    "utmEasting_bin0_array = xyz_bin0_array[:,0]\n",
    "utmNorthing_bin0_array = xyz_bin0_array[:,1]\n",
    "elevation_bin0_array = xyz_bin0_array[:,2]\n",
    "utmEasting_lastbin_array = xyz_lastbin_array[:,0]\n",
    "utmNorthing_lastbin_array = xyz_lastbin_array[:,1]\n",
    "elevation_lastbin_array = xyz_lastbin_array[:,2]\n",
    "\n",
    "print('')\n",
    "print('Processing Step Complete')\n",
    "print(end_time1 - start_time1)\n",
    "print('')\n",
    "\n",
    "print(number_of_pulses)\n",
    "print(np.min(x_origin_array),np.mean(x_origin_array),np.max(x_origin_array))\n",
    "print(np.min(y_origin_array),np.mean(y_origin_array),np.max(y_origin_array))\n",
    "print(np.min(z_origin_array),np.mean(z_origin_array),np.max(z_origin_array))\n",
    "print(np.min(dx_array),np.mean(dx_array),np.max(dx_array))\n",
    "print(np.min(dy_array),np.mean(dy_array),np.max(dy_array))\n",
    "print(np.min(dz_array),np.mean(dz_array),np.max(dz_array))\n",
    "print(np.min(utmEasting_bin0_array),np.mean(utmEasting_bin0_array),np.max(utmEasting_bin0_array))\n",
    "print(np.min(utmNorthing_bin0_array),np.mean(utmNorthing_bin0_array),np.max(utmNorthing_bin0_array))\n",
    "print(np.min(elevation_bin0_array),np.mean(elevation_bin0_array),np.max(elevation_bin0_array))\n",
    "print(np.min(utmEasting_lastbin_array),np.mean(utmEasting_lastbin_array),np.max(utmEasting_lastbin_array))\n",
    "print(np.min(utmNorthing_lastbin_array),np.mean(utmNorthing_lastbin_array),np.max(utmNorthing_lastbin_array))\n",
    "print(np.min(elevation_lastbin_array),np.mean(elevation_lastbin_array),np.max(elevation_lastbin_array))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954c3d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Subset waveforms to area of interest. Use waveform_aoi in all future processing when using pulsenumbers (iPulse)'''\n",
    "\n",
    "x_ref = 731937.0\n",
    "y_ref = 4713412.0\n",
    "dist_threshold = 50.0\n",
    "\n",
    "waveform_aoi = np.argwhere(((utmEasting_bin0_array >= x_ref - dist_threshold) & (utmEasting_bin0_array <= x_ref + dist_threshold) & (utmNorthing_bin0_array >= y_ref - dist_threshold) & (utmNorthing_bin0_array <= y_ref + dist_threshold)) | ((utmEasting_lastbin_array >= x_ref - dist_threshold) & (utmEasting_lastbin_array <= x_ref + dist_threshold) & (utmNorthing_lastbin_array >= y_ref - dist_threshold) & (utmNorthing_lastbin_array <= y_ref + dist_threshold)))\n",
    "waveform_aoi = np.reshape(waveform_aoi,len(waveform_aoi))\n",
    "num_pulses = len(waveform_aoi)   \n",
    "\n",
    "print(number_of_pulses)\n",
    "print(num_pulses)\n",
    "print(np.min(utmEasting_bin0_array[waveform_aoi]),np.mean(utmEasting_bin0_array[waveform_aoi]),np.max(utmEasting_bin0_array[waveform_aoi]))\n",
    "print(np.min(utmNorthing_bin0_array[waveform_aoi]),np.mean(utmNorthing_bin0_array[waveform_aoi]),np.max(utmNorthing_bin0_array[waveform_aoi]))\n",
    "print(np.min(elevation_bin0_array[waveform_aoi]),np.mean(elevation_bin0_array[waveform_aoi]),np.max(elevation_bin0_array[waveform_aoi]))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16797fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_waveform_location_information(waveform_x_axis, waveform_y_axis, waveform_z_axis, return_peak_locations):\n",
    "    ground_return_index = np.argmin(waveform_z_axis)\n",
    "    max_return_index = np.argmax(waveform_z_axis)\n",
    "    \n",
    "    ground_easting_coord = waveform_x_axis[ground_return_index]\n",
    "    ground_northing_coord = waveform_y_axis[ground_return_index]\n",
    "    max_easting_coord = waveform_x_axis[max_return_index]\n",
    "    max_northing_coord = waveform_y_axis[max_return_index]\n",
    "    peak_easting_coord = waveform_x_axis[return_peak_locations]\n",
    "    peak_northing_coord = waveform_y_axis[return_peak_locations]\n",
    "\n",
    "    min_easting_coord = min(waveform_x_axis)\n",
    "    max_easting_coord = max(waveform_x_axis)\n",
    "    min_northing_coord = min(waveform_y_axis)\n",
    "    max_northing_coord = max(waveform_y_axis)\n",
    "    \n",
    "    print(f\"Ground return Easting {ground_easting_coord}, Northing: {ground_northing_coord}\")\n",
    "    print(f\"Max height return Easting {max_easting_coord}, Northing: {max_northing_coord}\")\n",
    "    print(f\"Peak location easting: {peak_easting_coord}, northing: {peak_northing_coord}\")\n",
    "    print(f\"Min easting value: {min_easting_coord}, Max easting value: {max_easting_coord}\")\n",
    "    print(f\"Min northing value: {min_northing_coord}, Max northing value: {max_northing_coord}\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c04f8037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_cumulative_return_energy(waveform_intensity_x, waveform_elevation_y, plot):\n",
    "    '''Normalize cumulative return energy from waveform intensity values, with x being normalized intensity and y being elevation in meters.\n",
    "       Returns a 2D stacked array with format [intensity1, elevation1, intensity2, elevation2, ...]'''\n",
    "\n",
    "    # Create normalized cumulative signal energy from waveform intensity values\n",
    "    cumulative_energy_RH_boundaries = [0.25, 0.5, 0.75, 1.0]\n",
    "    \n",
    "    # Turn elevation into relative height\n",
    "    min_elevation = np.min(waveform_elevation_y)\n",
    "    relative_elevations = waveform_elevation_y - min_elevation\n",
    "\n",
    "    # Sort intensity by elevation\n",
    "    combined_data = np.array(list(zip(relative_elevations, waveform_intensity_x)))\n",
    "    sorted_data = combined_data[combined_data[:, 0].argsort()] \n",
    "    \n",
    "    # Extract sorted intensity and elevation\n",
    "    sorted_elevation = sorted_data[:, 0]\n",
    "    sorted_intensity = sorted_data[:, 1]\n",
    "\n",
    "    # Plot cumulative intensity\n",
    "    cumulative_intensity_sum = np.cumsum(sorted_intensity)\n",
    "    normalized_cumulative_intensity = cumulative_intensity_sum / cumulative_intensity_sum[-1]\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(normalized_cumulative_intensity, sorted_elevation, label=\"Normalized Cumulative Signal Level\")\n",
    "    \n",
    "        plt.xlabel(\"Normalized Cumulative Signal Level (DN)\")\n",
    "        plt.ylabel(\"Elevation (m)\")\n",
    "        plt.title(\"Normalized Cumulative Signal Level\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # Stack along the row to feed into future functions\n",
    "    stacked_intensity_elevation = np.stack((normalized_cumulative_intensity, sorted_elevation), axis=1).ravel()\n",
    "    return stacked_intensity_elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c13410f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_waveform_values(waveform, plot):\n",
    "    '''Interpolate waveform values so that there is 41 total cumulative signal levels and corresponding elevations from 0, 0.025, 0.05 ... 1 \n",
    "       Needed because all waveform inputs into k-means must be the same size'''\n",
    "\n",
    "    outlier = False\n",
    "    signal_raw_data = waveform[0::2]\n",
    "    elevation_raw_data = waveform[1::2]\n",
    "\n",
    "    # Connect the values together with straight lines (linear interpolate)\n",
    "    connected_function = interp1d(signal_raw_data, elevation_raw_data, kind='linear', bounds_error=False, fill_value=(elevation_raw_data[0], elevation_raw_data[-1]))\n",
    "\n",
    "    # Set normalized cumulative signal levels from 0-1 with 0.025 increments\n",
    "    interpolated_signal_levels = np.linspace(0, 1, 41)\n",
    "    interpolated_elevation_levels = connected_function(interpolated_signal_levels)\n",
    "\n",
    "    # Handle 0 explicitly\n",
    "    interpolated_elevation_levels[0] = 0.0\n",
    "\n",
    "    # Flag large trees for future outlier filtering\n",
    "    if interpolated_elevation_levels[-1] > 150:\n",
    "        print(\"Too big of a tree for current classification\")\n",
    "        outlier = True\n",
    "\n",
    "    if plot:\n",
    "        plt.plot(signal_raw_data, elevation_raw_data, 'o', label='Original Data')        \n",
    "        plt.plot(interpolated_signal_levels, interpolated_elevation_levels, '-', label='Interpolated Data')  \n",
    "        plt.xlabel('Normalized Cumulative Signal Levels')\n",
    "        plt.ylabel('Relative Elevation')\n",
    "        plt.title('Interpolation')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "    \n",
    "    interpolated_stacked_intensity_elevation = np.stack((interpolated_signal_levels, interpolated_elevation_levels), axis=1).ravel()\n",
    "    return interpolated_stacked_intensity_elevation, outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2f1d113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_silhouette_score(kmeans_fitted, X_train, k):\n",
    "    '''Calculate silhouette score: 1 is best, 0 means clusters are too close together, -1 means assigned to wrong cluster.\n",
    "       Metric to find out best k-value for k-means clustering and evaluate the model performance'''\n",
    "    \n",
    "    silhouette_avg = silhouette_score(X_train, kmeans_fitted.labels_)  \n",
    "            \n",
    "    print(\"K-value\", k)\n",
    "    print(\"Silhouette Score:\", silhouette_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af793e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cumulative_signal_slopes(interpolated_waveforms, min_signal_level, max_signal_level):\n",
    "    ''' Calculate slope of waveforms from min_signal_level to the max_signal_level and returns array with slopes of each waveform\n",
    "        Used as a LiDAR metric in some studies'''\n",
    "\n",
    "    waveform_slopes = []\n",
    "    for waveform in interpolated_waveforms:\n",
    "        signal_x = waveform[0::2]\n",
    "        elevation_y = waveform[1::2]\n",
    "\n",
    "        min_index = np.where(signal_x == min_signal_level)[0][0]\n",
    "        max_index = np.where(signal_x == max_signal_level)[0][0]\n",
    "\n",
    "        final_signal_x = signal_x[min_index:max_index+1]\n",
    "        final_elevation_y = elevation_y[min_index:max_index+1]\n",
    "        slope, intercept = np.polyfit(final_signal_x, final_elevation_y, 1)\n",
    "        waveform_slopes.append(slope)\n",
    "        \n",
    "    return waveform_slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cef4ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_waveform_normality(interpolated_waveforms, waveform):\n",
    "    '''See if elevation, signal level, and slopes are normally distributed for z-score outlier analysis using Q-Q Plots'''\n",
    "\n",
    "    if waveform:\n",
    "        one_d_waveforms = interpolated_waveforms.flatten()\n",
    "        elevation = one_d_waveforms[1::2]\n",
    "        cum_signal_level = one_d_waveforms[0::2]\n",
    "        \n",
    "        plt.figure(figsize=(6, 6))\n",
    "        stats.probplot(elevation, dist=\"norm\", plot=plt)\n",
    "        plt.title(\"Q-Q Plot for Relative Elevation\")\n",
    "        plt.show()\n",
    "    \n",
    "        plt.figure(figsize=(6, 6))\n",
    "        stats.probplot(cum_signal_level, dist=\"norm\", plot=plt)\n",
    "        plt.title(\"Q-Q Plot for Signal Level\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        stats.probplot(interpolated_waveforms, dist=\"norm\", plot=plt)\n",
    "        plt.title(\"Q-Q Plot for Waveform Slopes\")\n",
    "        plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5f5334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_normal_waveform_outlier_analysis(waveform_slopes):\n",
    "    '''Calculate Z-score analysis and identify outliers 3 z-scores away'''\n",
    "\n",
    "    z_score_threshold = 3\n",
    "    z_scores = stats.zscore(waveform_slopes)\n",
    "    \n",
    "    outliers = np.where((z_scores > z_score_threshold) | (z_scores < -z_score_threshold))\n",
    "    return outliers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca8a514f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_neighbors_distances(interpolated_waveforms, k):\n",
    "    '''Calculate mean neighbors distance to determine epsilon values for DBSCAN clustering'''\n",
    "    \n",
    "    neighbors = NearestNeighbors(n_neighbors=k)\n",
    "    neighbors_fit = neighbors.fit(interpolated_waveforms)\n",
    "    distances, indices = neighbors_fit.kneighbors(interpolated_waveforms)\n",
    "\n",
    "    distances = np.mean(distances[:,1:],axis=1)\n",
    "    distances = np.sort(distances)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.title(\"Average Distance from a point to k-nearest neighbors\")\n",
    "    \n",
    "    # Limit x values - different for every graph\n",
    "    plt.xlim(14000, 16000)\n",
    "    plt.plot(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7405abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dbscan_outlier_detection(interpolated_waveforms, min_pts, episilon):\n",
    "    '''Calculate DBSCAN to prune outliers on waveform data\n",
    "       min_pts depends on how many clusters in k-mean clustering/classes total'''\n",
    "    \n",
    "    dbscan = DBSCAN(eps=episilon, min_samples=min_pts)\n",
    "    dbscan.fit(interpolated_waveforms)\n",
    "    labels = dbscan.labels_\n",
    "\n",
    "    # Outliers occur where the labels are -1\n",
    "    outliers = np.where(labels == -1)[0]\n",
    "\n",
    "    waveform_outliers = interpolated_waveforms[outliers]\n",
    "    waveform_non_outliers = np.delete(interpolated_waveforms, outliers)\n",
    "    print(waveform_outliers)\n",
    "    plt.figure(figsize=(8,8))\n",
    "    for waveform in waveform_non_outliers:\n",
    "        plt.plot(waveform[0::2], waveform[1::2], c=labels)\n",
    "    for waveform in waveform_outliers:\n",
    "        plt.plot(waveform[0::2], waveform[1::2], c=\"red\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14acc600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_k_means_cluster_cumulative_returns(interpolated_waveforms_train, k_list, evaluate_elbow):\n",
    "    '''Return a trained k_means_cluster object for every k in the k-list'''\n",
    "    \n",
    "    k_means_classifiers = []\n",
    "    sse = []\n",
    "    \n",
    "    for k in k_list:\n",
    "        kmeans = KMeans(n_clusters=k, n_init=\"auto\", random_state=42).fit(interpolated_waveforms_train) \n",
    "        k_means_classifiers.append(kmeans)\n",
    "        sse.append(kmeans.inertia_)\n",
    "\n",
    "    if evaluate_elbow:\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        \n",
    "        plt.xlabel(r'Number of clusters k')\n",
    "        plt.ylabel('Log Sum of squared distance')\n",
    "        plt.plot(k_list, np.log(sse), '-o')\n",
    "        plt.show()\n",
    "        \n",
    "    return k_means_classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45b75a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_k_means_cluster_cumulative_returns(k_means_classifiers, X_test, k_list, plot_raw, evaluate_metrics):\n",
    "    '''K-means cluster waveforms based on trained classifiers and return cluster centers for waveforms and cluster assignments\n",
    "       Each is a 2D array with one dimension being the for each k_list and one for actual values\n",
    "       Return cluster centers and cluster assignments'''\n",
    "\n",
    "    cluster_centers = []\n",
    "    test_cluster_assignments = []\n",
    "    \n",
    "    if plot_raw:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        for waveform in X_test:\n",
    "            # Intensity is all even, relative elevation is all odd\n",
    "            plt.plot(waveform[0::2], waveform[1::2])\n",
    "            \n",
    "        plt.xlabel(\"Normalized Cumulative Signal Level (DN)\")\n",
    "        plt.ylabel(\"Elevation (m)\")\n",
    "        plt.title(\"Raw Waveform Values\")\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        \n",
    "    for classifier in k_means_classifiers:\n",
    "        test_cluster_assignments.append(classifier.predict(X_test))\n",
    "        cluster_centers.append(classifier.cluster_centers_)\n",
    "        \n",
    "    if evaluate_metrics:\n",
    "        for i, k in enumerate(k_list):\n",
    "            silhouette_avg = silhouette_score(X_test, test_cluster_assignments[i])  \n",
    "    \n",
    "            # Calculate davies_bouldin_index: lower value indicates better clustering (good separation and compactness)\n",
    "            # Higher value suggests poor clustering (overlapping or poorly defined clusters)\n",
    "            # Better for overall cluster comparisons\n",
    "            db_index = davies_bouldin_score(X_test, test_cluster_assignments[i])\n",
    "            print(f\"K: {k} Silhouette Score: {silhouette_avg}, Davies Bouldin Score: {db_index}\")\n",
    "    \n",
    "    return cluster_centers, test_cluster_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91f0ccb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_apparent_individual_biomass_information(apparent_individual_table, mapping_tagging_table, resolution, debug):\n",
    "    '''Return coordinates and total amount of apparent individuals in the entire plot based on plot resolution (1m, 3m, 10m, 25m, 50m, 100m)\n",
    "       Based on diagram found in NEON vegetation structure: https://data.neonscience.org/api/v0/documents/NEON_vegStructure_userGuide_vE?inline=true'''\n",
    "\n",
    "    individual_df = pd.read_csv(apparent_individual_table)\n",
    "    mapping_df = pd.read_csv(mapping_tagging_table)\n",
    "    total_data_count = 0\n",
    "    northing_arr = []\n",
    "    easting_arr = []\n",
    "    individual_id_arr = []\n",
    "    \n",
    "    for row in individual_df.itertuples(index=True, name=\"Row\"):\n",
    "        # Subplot resolution is always the 2nd number in subplotID\n",
    "        subplot_resolution = row.subplotID.split(\"_\")[1]\n",
    "        stemDiameter = row.stemDiameter\n",
    "        indiv_id = row.individualID\n",
    "        \n",
    "        # Check if subplot meets certain criteria to determine geolocation\n",
    "        if subplot_resolution.isdigit() and int(subplot_resolution) == resolution and not np.isnan(stemDiameter):\n",
    "            northing = row.adjNorthing\n",
    "            easting = row.adjEasting\n",
    "            indiv_id = row.individualID\n",
    "            \n",
    "            northing_arr.append(northing)\n",
    "            easting_arr.append(easting)\n",
    "            individual_id_arr.append(indiv_id)\n",
    "            total_data_count += 1\n",
    "            if debug:\n",
    "                print(f\"Northing: {northing}, Easting: {easting}, ID: {indiv_id}\")\n",
    "            \n",
    "        # Otherwise check if individual geolocation information is valid (tree is mapped)\n",
    "        elif not np.isnan(stemDiameter) and row.individualID in mapping_df[\"individualID\"].values:\n",
    "            mapping_data = mapping_df.loc[mapping_df[\"individualID\"] == indiv_id]\n",
    "            northing = mapping_data[\"adjNorthing\"].iloc[0]\n",
    "            easting = mapping_data[\"adjEasting\"].iloc[0]\n",
    "\n",
    "            if not np.isnan(northing):\n",
    "                northing_arr.append(northing)\n",
    "                easting_arr.append(easting)\n",
    "                individual_id_arr.append(indiv_id)\n",
    "                total_data_count +=1\n",
    "                if debug:\n",
    "                    print(f\"Northing: {northing}, Easting: {easting}, ID: {indiv_id}\")\n",
    "                    print(\"Mapped\")\n",
    "                \n",
    "    print(f\"Total individual data count: {total_data_count}\")\n",
    "    return np.array(northing_arr), np.array(easting_arr), np.array(individual_id_arr), total_data_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78929a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_indv_tree_biomass(genus, family, species, dbh):\n",
    "    '''Lookup table for tree biomass based on DBH (cm) and genus, family, and species in Chojnacky paper\n",
    "       https://www.fs.usda.gov/nrs/pubs/jrnl/2014/nrs_2014_chojnacky_001.pdf\n",
    "       Some b0 and b1 are interpolated based on genus if species does not exist'''\n",
    "    \n",
    "    b0 = 0\n",
    "    b1 = 0\n",
    "\n",
    "    # 45 conifer species\n",
    "    if genus == \"Abies\":\n",
    "        if species == \"balsamea\" or species == \"fraseri\" or species == \"lasiocarpa\":\n",
    "            # Abies < 0.35 spg\n",
    "            b0 = -2.3123\n",
    "            b1 = 2.3482\n",
    "        else:\n",
    "            # Abies >= 0.35 spg, intepolation for unknown species\n",
    "            b0 = -3.1774\n",
    "            b1 = 2.6426\n",
    "    elif genus == \"Thuja\":\n",
    "        if species == \"occidentalis\":\n",
    "            # Cupressaceae < 0.30 spg\n",
    "            b0 = -1.9615\n",
    "            b1 = 2.1063\n",
    "        elif species == \"plicata\":\n",
    "            # Cupressaceae 0.3 - 0.39 spg\n",
    "            b0 = -2.7765\n",
    "            b1 = 2.4195\n",
    "        else:\n",
    "            print(\"No species B0 and B1 found\")\n",
    "    elif genus == \"Calocedrus\" or genus == \"Sequoiadendron\":\n",
    "        # Cupressaceae 0.3 - 0.39 spg\n",
    "        b0 = -2.7765\n",
    "        b1 = 2.4195  \n",
    "    elif genus == \"Chamaecyparis\" or genus == \"Juniperus\":\n",
    "        # Cupressaceae >= 0.4 spg\n",
    "        b0 = -2.6327\n",
    "        b1 = 2.4757\n",
    "    elif genus == \"Larix\":\n",
    "        # Larix\n",
    "        b0 = -2.3012\n",
    "        b1 = 2.3853\n",
    "    elif genus == \"Picea\":\n",
    "        if species == \"engelmannii\" or species == \"sitchensis\":\n",
    "            # Picea < 0.35 spg\n",
    "            b0 = -3.03\n",
    "            b1 = 2.5567\n",
    "        else:\n",
    "            # Picea >= 0.35 spg, interpolation for unkown species\n",
    "            b0 = -2.1364\n",
    "            b1 = 2.3233\n",
    "    elif genus == \"Pinus\":\n",
    "        if species == \"strobus\" or species == \"echinata\" or species == \"elliottii\" or species == \"palustris\" or species == \"rigida\" or species == \"taeda\":\n",
    "            # Pinus >= 0.45 spg\n",
    "            b0 = -3.0506\n",
    "            b1 = 2.6465\n",
    "        else:\n",
    "            # Pinus < 0.45 spg, interpolation for unknown species\n",
    "            b0 = -2.6177\n",
    "            b1 = 2.4638\n",
    "    elif genus == \"Pseudotsuga\":\n",
    "        # Pseudotsuga\n",
    "        b0 = -2.4623\n",
    "        b1 = 2.4852\n",
    "    elif genus == \"Tsuga\":\n",
    "        if species == \"canadensis\":\n",
    "            # Tsuga < 0.4 spg\n",
    "            b0 = -2.3480\n",
    "            b1 = 2.3876\n",
    "        elif species == \"heterophylla\" or species == \"mertensiana\":\n",
    "            # Tsuga >= 0.4 spg\n",
    "            b0 = -2.9208\n",
    "            b1 = 2.5697\n",
    "        else:\n",
    "            print(\"No species B0 and B1 found\")\n",
    "            \n",
    "    # 70 hardwood species taxa, based on family\n",
    "    elif family == \"Aceraceae\":\n",
    "        if species == \"saccharum\":\n",
    "            # Aceraceae >= 0.5 spg\n",
    "            b0 = -1.8011\n",
    "            b1 = 2.3852\n",
    "        else:\n",
    "            # Aceraceae < 0.5 spg, interpolation\n",
    "            b0 = -2.0470\n",
    "            b1 = 2.3852\n",
    "    elif family == \"Betulaceae\":\n",
    "        if genus == \"Alnus\":\n",
    "            # Betulaceae < 0.4 spg\n",
    "            b0 = -2.5932\n",
    "            b1 = 2.5349\n",
    "        elif species == \"papyrifera\" or species == \"populifolia\":\n",
    "            # Betulaceae 0.4 - 0.49 spg\n",
    "            b0 = -2.2271\n",
    "            b1 = 2.4513\n",
    "        elif species == \"alleghaniensis\":\n",
    "            # Betulaceae 0.5 - 0.59\n",
    "            b0 = -1.8096\n",
    "            b1 = 2.3480\n",
    "        elif species == \"lenta\" or genus == \"Ostrya\":\n",
    "            # Betulaceae >= 0.6 spg\n",
    "            b0 = -2.2652\n",
    "            b1 = 2.5349\n",
    "        else:\n",
    "            # Specific gravity > 0.6 used for interpolation\n",
    "            b0 = -2.2652\n",
    "            b1 = 2.5349\n",
    "    elif family == \"Cornaceae\" or family == \"Ericaceae\" or family == \"Lauraceae\" or family == \"Platanaceae\" or family == \"Rosaceae\" or family == \"Ulmaceae\":\n",
    "        b0 = -2.2118\n",
    "        b1 = 2.4133\n",
    "    elif family == \"Juglandaceae\":\n",
    "        # Fabaceae/Juglandaceae Carya\n",
    "        b0 = -2.5095\n",
    "        b1 = 2.6175\n",
    "    elif family == \"Fabaceae\":\n",
    "        # Fabaceae other\n",
    "        b0 = -2.5095\n",
    "        b1 = 2.5437\n",
    "    elif family == \"Fagaceae\":\n",
    "        # Evergreen Fagaceae (e.g. Quercus chrysolepis, Q. laurifolia, Lithocarpus, etc.)\n",
    "        evergreen_oaks = [\"chrysolepis\",\"densiflorus\",\"douglasii\",\"laurifolia\",\"minima\"]\n",
    "        if species in evergreen_oaks:\n",
    "            b0 = -2.2189\n",
    "            b1 =  2.4410\n",
    "        else:\n",
    "            # Deciduous Fagaceae (e.g. Quercus alba, Q. rubra, etc., Castanea, Fagus)\n",
    "            b0 = -2.0705\n",
    "            b1 =  2.4410  \n",
    "    elif family == \"Hamamelidaceae\":\n",
    "        b0 = -2.6390\n",
    "        b1 =  2.5466\n",
    "    elif family in [\"Hippocastanaceae\", \"Tiliaceae\"]:\n",
    "        b0 = -2.4108\n",
    "        b1 =  2.1777\n",
    "    elif family == \"Magnoliaceae\":\n",
    "        b0 = -2.5497\n",
    "        b1 =  2.5011\n",
    "    elif family == \"Oleaceae\":\n",
    "        if species == \"americana\":\n",
    "            # Oleaceae >= 0.55 spg\n",
    "            b0 = -1.8384\n",
    "            b1 =  2.4950\n",
    "        else:\n",
    "            b0 = -2.0314\n",
    "            b1 = 2.3524\n",
    "    elif family == \"Salicaceae\":\n",
    "        poplar_spg_map = {\n",
    "            \"balsamifera\":   0.31,\n",
    "            \"trichocarpa\":   0.31,\n",
    "            \"spp\":           0.34, \n",
    "            \"deltoides\":     0.37,\n",
    "            \"grandidentata\": 0.36,\n",
    "            \"tremuloides\":   0.35\n",
    "        }\n",
    "        willow_spg_map = {\n",
    "            \"alba\": 0.36,\n",
    "            \"spp\":  0.36\n",
    "        }\n",
    "\n",
    "        if genus == \"Populus\":\n",
    "            spg_val = poplar_spg_map.get(species, 0.34)\n",
    "        elif genus == \"Salix\":\n",
    "            spg_val = willow_spg_map.get(species, 0.36)\n",
    "        else:\n",
    "            spg_val = 0.36 \n",
    "\n",
    "        if spg_val < 0.35:\n",
    "            b0 = -2.6683\n",
    "            b1 =  2.4561\n",
    "        else:\n",
    "            b0 = -2.4441\n",
    "            b1 =  2.4291      \n",
    "            \n",
    "    # 15 Woodland species, uses drc instead of dbh\n",
    "    elif family == \"Cupressaceae\":\n",
    "        # e.g. Cupressus spp, Juniperus monosperma/occidentalis/osteosperma, etc.\n",
    "        b0 = -2.7096\n",
    "        b1 =  2.1942\n",
    "    elif family in [\"Fabaceae\", \"Rosaceae\"]:\n",
    "        # Fabaceae or Rosaceae group\n",
    "        b0 = -2.9255\n",
    "        b1 =  2.4109\n",
    "    elif family == \"Fagaceae\":\n",
    "        # Fagaceae group\n",
    "        b0 = -3.0304\n",
    "        b1 =  2.4982\n",
    "    elif family == \"Pinaceae\":\n",
    "        # Pinaceae group\n",
    "        b0 = -3.2007\n",
    "        b1 =  2.5339\n",
    "\n",
    "    else:\n",
    "        # The tree/shrub doesn't match any of the species\n",
    "        print(\"Error: The tree/shrub doesn't match any of the species\")\n",
    "    return b0, b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da11265c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_tree_biomass(northing_area, easting_area, resolution, mapping_tagging_table, apparent_individual_table):\n",
    "    \"\"\"\n",
    "    Calculate total tree biomass for a given area using species lookup table, geolocation data in a certain mxm resolution.\n",
    "    Mapping and tagging and apparent individual table is taken from NEON.\n",
    "    \"\"\"\n",
    "\n",
    "    biomass_data = []\n",
    "    individual_df = pd.read_csv(apparent_individual_table)\n",
    "    mapping_df = pd.read_csv(mapping_tagging_table)\n",
    "\n",
    "    # Filter all mapping and tagging points to be within the northing area and easting area (bounding box)\n",
    "    northing_range_max = northing_area + resolution/2\n",
    "    northing_range_min = northing_area - resolution/2\n",
    "    easting_range_max = easting_area + resolution/2\n",
    "    easting_range_min = easting_area - resolution/2\n",
    "\n",
    "    mapping_df_filtered = mapping_df[(mapping_df[\"adjNorthing\"] >= northing_range_min) & (mapping_df[\"adjNorthing\"] <= northing_range_max) \n",
    "                                    & (mapping_df[\"adjEasting\"] >= easting_range_min) & (mapping_df[\"adjEasting\"] <= easting_range_max)]\n",
    "    \n",
    "    not_found_count = 0\n",
    "    lost_na_data_count = 0\n",
    "    for row in mapping_df_filtered.itertuples(index=True, name=\"Row\"):\n",
    "        # Search mapping and tagging to get precise geolocation and species for individual table values\n",
    "        if row.individualID in individual_df[\"individualID\"].values:\n",
    "            genus = row.genus\n",
    "            family = row.family\n",
    "            species = row.scientificName.split(\" \")[1]\n",
    "\n",
    "            # Get diameter breast height from individual table and check if tree is lost (no data)\n",
    "            individual_data = individual_df.loc[individual_df[\"individualID\"] == row.individualID]\n",
    "            dbh = individual_data[\"stemDiameter\"].iloc[0]\n",
    "            measurement_height = individual_data[\"measurementHeight\"].iloc[0]\n",
    "\n",
    "            if np.isnan(dbh):\n",
    "                lost_na_data_count += 1\n",
    "                print(\"Tree data lost or invalid\")\n",
    "                continue\n",
    "\n",
    "            if measurement_height < 130:\n",
    "                print(\"DRC instead of DBH due to smaller tree size\")\n",
    "            b0, b1 = calculate_indv_tree_biomass(genus, family, species, dbh)\n",
    "\n",
    "            # Based on Chojnacky 2013\n",
    "            biomass_ln = b0 + b1 * np.log(dbh)\n",
    "            raw_biomass = np.exp(biomass_ln)\n",
    "            biomass_data.append(raw_biomass)\n",
    "            \n",
    "        else:\n",
    "            not_found_count += 1\n",
    "            print(\"Mapping and tagging not found in individual table\")\n",
    "\n",
    "    total_biomass = np.sum(biomass_data)\n",
    "    return total_biomass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62a7129b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_associated_biomass_waveform_group(waveform_x_axis, waveform_y_axis, grouped_biomass, shot_number):\n",
    "    '''\n",
    "    Check if waveform is within a biomass group, groups are defined by biomass individuals spanning a certain area.\n",
    "    Run this function first to see if a cluster of waveforms has an associated biomass value before all other biomass functions.\n",
    "    '''\n",
    "\n",
    "    biomass_found = False\n",
    "    min_waveform_easting = np.min(waveform_x_axis)\n",
    "    max_waveform_easting = np.max(waveform_x_axis)\n",
    "    min_waveform_northing = np.min(waveform_y_axis)\n",
    "    max_waveform_northing = np.max(waveform_y_axis)\n",
    "\n",
    "    for index, group in enumerate(grouped_biomass):\n",
    "        if min_waveform_easting >= group[0] and max_waveform_easting <= group[1] and min_waveform_northing >= group[2] and max_waveform_northing <= group[3]:\n",
    "            biomass_found = True\n",
    "            print(f\"Waveform {shot_number} found in group {index + 1}\")\n",
    "    return biomass_found, group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c656bd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_associated_biomass_waveform_pulse_number(biomass_northing_arr, biomass_easting_arr, waveform_x_axis, waveform_y_axis, shot_number, debug):\n",
    "    '''\n",
    "    Find the exact biomass geolocation associated with each waveform based on waveform geolocation and biomass geolocation.\n",
    "    Precondition: Biomass northing and easting include all biomass points in the plot.\n",
    "    '''\n",
    "\n",
    "    min_waveform_easting = np.min(waveform_x_axis)\n",
    "    max_waveform_easting = np.max(waveform_x_axis)\n",
    "    min_waveform_northing = np.min(waveform_y_axis)\n",
    "    max_waveform_northing = np.max(waveform_y_axis)\n",
    "    waveform_easting = np.mean([min_waveform_easting, max_waveform_easting])\n",
    "    waveform_northing = np.mean([min_waveform_northing, max_waveform_northing])\n",
    "    waveform_easting_min = waveform_easting - 0.5\n",
    "    waveform_easting_max = waveform_easting + 0.5\n",
    "    waveform_northing_min = waveform_northing - 0.5\n",
    "    waveform_northing_max = waveform_northing + 0.5\n",
    "    biomass_found = False\n",
    "\n",
    "    # Stack biomass arrays for easier searching\n",
    "    paired_biomass = np.column_stack((biomass_northing_arr, biomass_easting_arr))\n",
    "    \n",
    "    # Search paired biomass arrays for waveform\n",
    "    biomass_index = np.where((paired_biomass[:, 0] >= waveform_northing_min) & (paired_biomass[:, 0] <= waveform_northing_max) & \n",
    "                             (paired_biomass[:,1] >= waveform_easting_min) & (paired_biomass[:, 1] <= waveform_easting_max))\n",
    "    if biomass_index[0].size > 0:\n",
    "        biomass_found = True\n",
    "        if debug:\n",
    "            print(f\"Waveform biomass found at {paired_biomass[biomass_index[0]]}\")\n",
    "            print(f\"Number of biomass values: {biomass_index[0].size}\")\n",
    "\n",
    "    return biomass_found, waveform_easting, waveform_northing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd69b446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_biomass_tree_heights(northing, easting, resolution, mapping_tagging_table, apparent_individual_table):\n",
    "    '''Find in-situ tree heights for each waveform from NEON apparent individual table based on geolocation data'''\n",
    "    \n",
    "    individual_df = pd.read_csv(apparent_individual_table)\n",
    "    mapping_df = pd.read_csv(mapping_tagging_table)\n",
    "\n",
    "    # Filter all mapping and tagging points to be within the northing area and easting area (bounding box)\n",
    "    northing_range_max = northing + resolution/2\n",
    "    northing_range_min = northing - resolution/2\n",
    "    easting_range_max = easting + resolution/2\n",
    "    easting_range_min = easting - resolution/2\n",
    "\n",
    "    mapping_df_filtered = mapping_df[(mapping_df[\"adjNorthing\"] >= northing_range_min) & (mapping_df[\"adjNorthing\"] <= northing_range_max) \n",
    "                                    & (mapping_df[\"adjEasting\"] >= easting_range_min) & (mapping_df[\"adjEasting\"] <= easting_range_max)]\n",
    "    \n",
    "    not_found_count = 0\n",
    "    height = -10\n",
    "    for row in mapping_df_filtered.itertuples(index=True, name=\"Row\"):\n",
    "        # Search mapping and tagging to get precise geolocation and species for individual table values\n",
    "        if row.individualID in individual_df[\"individualID\"].values:\n",
    "            individual_data = individual_df.loc[individual_df[\"individualID\"] == row.individualID]\n",
    "            height = individual_data[\"height\"].iloc[0]\n",
    "            \n",
    "        else:\n",
    "            not_found_count += 1\n",
    "            print(\"Mapping and tagging not found in individual table\")\n",
    "    return height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c50ca6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_waveform_data_row_with_biomass(waveform_file_name, interp_waveform, waveform_x_axis, waveform_y_axis, biomass_value, cluster_number, shot_number, biomass_group_number):\n",
    "    '''Helper function for main biomass function: Add waveform data row with biomass value to waveform biomass dataframe'''    \n",
    "\n",
    "    waveform_elevation_data = list(interp_waveform[1::2])\n",
    "    waveform_intensity_data = interp_waveform[0::2]\n",
    "\n",
    "    # Have a seperate dataframe for actual waveform elevation data vs the metadata\n",
    "    elevation_df = pd.DataFrame([waveform_elevation_data], columns=[f\"Elevation_{np.round(waveform_intensity_data[i], 3)}\" for i in range(len(waveform_elevation_data))])\n",
    "\n",
    "    waveform_biomass_df = pd.DataFrame({'FileName': [waveform_file_name], 'Pulsenumber': [shot_number], 'Biomass': [biomass_value], 'Easting': [waveform_x_axis], 'Northing': [waveform_y_axis], 'Group': [biomass_group_number], 'Cluster': [cluster_number]})\n",
    "    waveform_biomass_df = pd.concat([waveform_biomass_df, elevation_df], axis=1)\n",
    "    return waveform_biomass_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760788ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_test_train_cluster_indices(cluster_group_values, number_of_clusters, clusters_with_no_data, clusters_with_less_than_4, split_percentage):\n",
    "    '''Split data into train test for linear regression: 75%, 25% based on clusters'''    \n",
    "\n",
    "    train_indices_aggregate = []\n",
    "    test_indices_aggregate = []\n",
    "    \n",
    "    # Build a dictionary of {cluster number: [associated indices in elevation data]}\n",
    "    cluster_group_indices_train = {i: [] for i in range(number_of_clusters)}\n",
    "    cluster_group_indices_test = {i: [] for i in range(number_of_clusters)}\n",
    "\n",
    "    # Loop through each unique cluster\n",
    "    unique_clusters = np.unique(cluster_group_values)\n",
    "    for cluster in unique_clusters:\n",
    "        if cluster in clusters_with_no_data:\n",
    "            continue\n",
    "            \n",
    "        # Get indices for the current cluster group\n",
    "        cluster_indices = np.where(cluster_group_values == cluster)[0]\n",
    "        \n",
    "        if cluster in clusters_with_less_than_4:\n",
    "            cluster_group_indices_train[cluster].extend(cluster_indices)\n",
    "            continue\n",
    "            \n",
    "        train_idx, test_idx = train_test_split(cluster_indices, test_size=split_percentage, random_state=42)\n",
    "        \n",
    "        # Add the indices to the respective lists\n",
    "        train_indices_aggregate.extend(train_idx)\n",
    "        test_indices_aggregate.extend(test_idx)\n",
    "        cluster_group_indices_train[cluster].extend(train_idx)\n",
    "        cluster_group_indices_test[cluster].extend(test_idx)\n",
    "\n",
    "    return train_indices_aggregate, test_indices_aggregate, cluster_group_indices_train, cluster_group_indices_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c47441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_general_linear_regression(waveform_canopy_heights, biomass_values, waveform_canopy_graph_heights, graph_title):\n",
    "    '''Fit one linear regression model with all waveform based on waveform metrics and field AGB values'''\n",
    "    \n",
    "    regression_model = LinearRegression().fit(waveform_canopy_heights, biomass_values)\n",
    "    regression_predictions = regression_model.predict(waveform_canopy_heights)\n",
    "    \n",
    "    score = regression_model.score(waveform_canopy_heights, biomass_values)\n",
    "    rmse = np.sqrt(mean_squared_error(biomass_values, regression_predictions))\n",
    "    \n",
    "    print(f\"Intercept: {regression_model.intercept_}\")\n",
    "    print(f\"Coefficients: {regression_model.coef_}\")\n",
    "    print(f\"R^2: {score}\")\n",
    "    print(f\"RMSE: {rmse}\")\n",
    "\n",
    "    reg_model_graphing = LinearRegression().fit(waveform_canopy_graph_heights, biomass_values)\n",
    "    reg_graph_predictions = reg_model_graphing.predict(waveform_canopy_graph_heights)\n",
    "    sorted_indices = np.argsort(waveform_canopy_graph_heights.flatten())\n",
    "    sorted_waveform_canopy_heights = waveform_canopy_graph_heights.flatten()[sorted_indices]\n",
    "    sorted_regression_predictions = reg_graph_predictions[sorted_indices]\n",
    "    \n",
    "    fig, (ax1) = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    ax1.set_title(f'{graph_title}')\n",
    "    ax1.set_xlabel('Relative Height (m)')\n",
    "    ax1.set_ylabel('Biomass (kg)')\n",
    "    ax1.scatter(waveform_canopy_graph_heights, biomass_values, label='Canopy Heights')\n",
    "    ax1.plot(sorted_waveform_canopy_heights, sorted_regression_predictions, color='red', label='Regression Line')\n",
    "    ax1.legend()\n",
    "\n",
    "    fig.subplots_adjust(wspace=0.5)\n",
    "    plt.show()\n",
    "    return sorted_waveform_canopy_heights, sorted_regression_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bb33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustered_linear_regression(waveform_canopy_heights, biomass_values, number_of_clusters, cluster_group_values, clusters_with_no_data, clusters_with_limited_data, split_percentage, fit_line_x, fit_line_y):\n",
    "    '''Create a linear regression for each cluster and plot it'''\n",
    "    \n",
    "    # Use the indices to create train and test splits for each array\n",
    "    train_ind_aggr, test_ind_aggr, train_ind_cluster, test_ind_cluster = split_test_train_cluster_indices(cluster_group_values, number_of_clusters, clusters_with_no_data, clusters_with_limited_data, split_percentage)\n",
    "\n",
    "    fig, (ax1) = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    \n",
    "    # Create a linear regression for all the clusters with all data as training\n",
    "    for i in range(number_of_clusters):\n",
    "        train_test_indices_aggregate = np.concatenate((train_ind_cluster[i], test_ind_cluster[i]))\n",
    "\n",
    "        if i in clusters_with_no_data:\n",
    "            continue\n",
    "\n",
    "        # if i in clusters_with_limited_data:\n",
    "        #     fig, (ax1) = plt.subplots(1, 1, figsize=(10, 5))\n",
    "        #     ax1.set_title(f'Waveform Relative Height 97.5 vs. Biomass Cluster {i}')\n",
    "        #     ax1.set_xlabel('Relative Height (m)')\n",
    "        #     ax1.set_ylabel('Biomass (kg)')\n",
    "        #     ax1.scatter(waveform_canopy_heights[train_test_indices_aggregate], biomass_values[train_test_indices_aggregate])\n",
    "        #     continue\n",
    "\n",
    "        elevation_train_2 = waveform_canopy_heights[train_ind_cluster[i]]\n",
    "        elevation_test_2 = waveform_canopy_heights[test_ind_cluster[i]]\n",
    "        elevation_aggregate_2 = waveform_canopy_heights[train_test_indices_aggregate]\n",
    "            \n",
    "        cluster_train_2 = cluster_group_values[train_ind_cluster[i]]\n",
    "        cluster_test_2 = cluster_group_values[test_ind_cluster[i]]\n",
    "        \n",
    "        biomass_train_2 = biomass_values[train_ind_cluster[i]]\n",
    "        biomass_test_2 = biomass_values[test_ind_cluster[i]]\n",
    "        biomass_aggregate_2 = biomass_values[train_test_indices_aggregate]\n",
    "\n",
    "        regression_model_2 = LinearRegression().fit(elevation_aggregate_2, biomass_aggregate_2)\n",
    "        score_2 = regression_model_2.score(elevation_aggregate_2, biomass_aggregate_2)\n",
    "\n",
    "        print(f\"For cluster {i}\")\n",
    "        print(f\"Intercept: {regression_model_2.intercept_}\")\n",
    "        print(f\"Coefficients: {regression_model_2.coef_}\")\n",
    "        print(f\"Score: {score_2}\")\n",
    "\n",
    "        # Plot scatter points used to create regression in a different color\n",
    "        ax1.scatter(elevation_aggregate_2, biomass_aggregate_2, label=f\"Cluster {i}\")\n",
    "\n",
    "    ax1.set_title(f'Clustered Waveform Relative Height 97.5 Points vs. Biomass')\n",
    "    ax1.set_xlabel('Relative Height (m)')\n",
    "    ax1.set_ylabel('Biomass (kg)')\n",
    "    ax1.plot(fit_line_x, fit_line_y, color='red')\n",
    "    ax1.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a00661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_exponential_regression(waveform_canopy_heights_flattened, biomass_values):\n",
    "    '''Exponential regression in the form of a * np.exp(b * t) + c to RH and biomass'''\n",
    "    \n",
    "    popt, pcov = curve_fit(lambda t, a, b, c: a * np.exp(b * t) + c, \n",
    "                           waveform_canopy_heights_flattened, \n",
    "                           biomass_values)\n",
    "    a = popt[0]\n",
    "    b = popt[1]\n",
    "    c = popt[2]\n",
    "    exponential_predictions = a * np.exp(b * waveform_canopy_heights_flattened) + c\n",
    "\n",
    "    fig, (ax1) = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    ax1.set_title(f'Waveform Relative Height 97.5 and Exponential Relationship Plotted')\n",
    "    ax1.set_xlabel('Relative Height (m)')\n",
    "    ax1.set_ylabel('Biomass (kg)')\n",
    "    ax1.scatter(waveform_canopy_heights_flattened, biomass_values)\n",
    "    ax1.plot(np.sort(waveform_canopy_heights_flattened), exponential_predictions[np.argsort(waveform_canopy_heights_flattened)], color='red')\n",
    "\n",
    "    exponential_rmse = np.sqrt(np.mean((biomass_values - exponential_predictions)**2))\n",
    "    print(f\"Exponential RMSE: {exponential_rmse}\")\n",
    "    print(f\"A: {a}, B: {b}, C: {c}\")\n",
    "\n",
    "    return np.sort(waveform_canopy_heights_flattened), exponential_predictions[np.argsort(waveform_canopy_heights_flattened)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab04979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_regression_main(waveform_biomass_data_file):\n",
    "    '''Main function for regression analysis. Custom based on what is needed'''\n",
    "\n",
    "    waveform_biomass_data = pd.read_csv(waveform_biomass_data_file)\n",
    "    field_canopy_heights_df = pd.read_csv(\"/Users/felixyu/Documents/Remote-Sensing/Neon-Pls-Wvs-Las/data/NEON_field_canopy_heights.csv\")\n",
    "    \n",
    "    # DF Variables\n",
    "    waveforms_with_biomass = waveform_biomass_data.iloc[:, 10:]\n",
    "    waveform_elevation_data = waveforms_with_biomass.to_numpy()\n",
    "\n",
    "    cluster_group_values = waveform_biomass_data[\"Cluster\"].to_numpy()\n",
    "    biomass_values = np.array(waveform_biomass_data[\"Biomass\"])\n",
    "    canopy_top_values = waveform_biomass_data[\"CanopyTop\"].to_numpy()\n",
    "    canopy_bottom_values = waveform_biomass_data[\"CanopyBottom\"].to_numpy()\n",
    "    surface_elevation_values = waveform_biomass_data[\"SurfaceElevation\"].to_numpy()\n",
    "    field_canopy_heights = field_canopy_heights_df[\"FieldCanopyHeights\"].to_numpy()\n",
    "\n",
    "    # Cluster vars\n",
    "    number_of_clusters = 8\n",
    "    cluster_group_count = {i: 0 for i in range(number_of_clusters)}\n",
    "    clusters_with_no_data = [4]\n",
    "    clusters_with_limited_data = []\n",
    "    split_percentage = 0.25\n",
    "    \n",
    "    # Get metrics\n",
    "    mid_index = waveform_elevation_data.shape[1] // 2\n",
    "    quarter_index = waveform_elevation_data.shape[1] // 4\n",
    "    waveform_elevation_RH100 = waveform_elevation_data[:, [-1]]\n",
    "    waveform_elevation_RH10 = waveform_elevation_data[:, [5]]\n",
    "    waveform_elevation_RH975_RH100 = waveform_elevation_data[:, [-2, -1]]\n",
    "    waveform_elevation_RH975 = waveform_elevation_data[:, [-2]]\n",
    "    waveform_elevation_RH50 = waveform_elevation_data[:, [mid_index]]\n",
    "    waveform_elevation_RH50_RH975 = waveform_elevation_data[:, [mid_index, -2]]\n",
    "    waveform_elevation_RH10_RH975 = waveform_elevation_data[:, [5, -2]]\n",
    "    waveform_elevation_RH25_RH975 = waveform_elevation_data[:, [quarter_index, -2]]\n",
    "\n",
    "    waveform_RH_start = canopy_bottom_values - surface_elevation_values\n",
    "    waveform_canopy_RH975_flattened = waveform_elevation_RH975.flatten() + waveform_RH_start\n",
    "    \n",
    "    # Filter out waveforms with negative canopy heights and biomass values below 500 with a canopy height bigger than 20\n",
    "    non_neg_biomass_ch_mask = ((waveform_canopy_RH975_flattened < 20.0) | (biomass_values > 500)) & (waveform_RH_start >= 0)\n",
    "    waveform_RH_start_filtered = waveform_RH_start[non_neg_biomass_ch_mask]\n",
    "    waveform_elevation_RH100_filtered = waveform_elevation_RH100[non_neg_biomass_ch_mask, :]\n",
    "    waveform_elevation_RH10_filtered = waveform_elevation_RH10[non_neg_biomass_ch_mask, :]\n",
    "    waveform_elevation_RH975_filtered = waveform_elevation_RH975[non_neg_biomass_ch_mask, :]\n",
    "    waveform_elevation_RH50_filtered = waveform_elevation_RH50[non_neg_biomass_ch_mask, :]\n",
    "    cluster_group_values_filtered = cluster_group_values[non_neg_biomass_ch_mask]\n",
    "    field_canopy_heights_filtered = field_canopy_heights[non_neg_biomass_ch_mask]\n",
    "    biomass_values_masked = biomass_values[non_neg_biomass_ch_mask]\n",
    "    \n",
    "    waveform_RH_start_column = waveform_RH_start_filtered.reshape(-1, 1)\n",
    "    waveform_canopy_heights_RH100 = waveform_elevation_RH100_filtered.flatten() + waveform_RH_start_filtered\n",
    "    waveform_canopy_heights_RH975 = waveform_RH_start_column + waveform_elevation_RH975_filtered\n",
    "    waveform_canopy_heights_RH975_flattened = waveform_canopy_heights_RH975.flatten()\n",
    "    waveform_canopy_heights_RH10 = waveform_RH_start_column + waveform_elevation_RH10_filtered\n",
    "    waveform_canopy_heights_RH50 = waveform_RH_start_column + waveform_elevation_RH50_filtered\n",
    "    waveform_canopy_heights_RH10_RH975 = np.column_stack((waveform_canopy_heights_RH10, waveform_canopy_heights_RH975))\n",
    "    waveform_canopy_heights_RH50_RH975 = np.column_stack((waveform_canopy_heights_RH50, waveform_canopy_heights_RH975))\n",
    "\n",
    "    # Get total amount of values in each cluster\n",
    "    for cluster in cluster_group_values_filtered:\n",
    "        cluster_group_count[cluster] += 1\n",
    "        \n",
    "    print(cluster_group_count)\n",
    "\n",
    "    # Graph field data vs waveform data\n",
    "    fig, (ax1) = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    ax1.set_title(f'Waveform Heights vs Field Heights')\n",
    "    ax1.set_xlabel('Waveform Relative Height (m)')\n",
    "    ax1.set_ylabel('Field Relative Height (m)')\n",
    "    ax1.set_xlim(0, 30)\n",
    "    ax1.set_ylim(0, 30)\n",
    "    ax1.set_xticks(range(0, 31, 5))\n",
    "    ax1.set_yticks(range(0, 31, 5))\n",
    "\n",
    "    ax1.scatter(waveform_canopy_heights_RH975_flattened, field_canopy_heights_filtered, label='Tree Heights')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Fit one linear regression model with all waveforms\n",
    "    general_reg_graph_title = \"Waveform RH97.5/Canopy Height Vs. Biomass\"\n",
    "    \n",
    "    perform_general_linear_regression(waveform_canopy_heights_RH975, biomass_values_masked, waveform_canopy_heights_RH975, general_reg_graph_title)\n",
    "    exponential_fit_line_x, exponential_fit_line_y = perform_exponential_regression(waveform_canopy_heights_RH975_flattened, biomass_values_masked)\n",
    "    perform_clustered_linear_regression(waveform_canopy_heights_RH975, biomass_values_masked, number_of_clusters, cluster_group_values_filtered, clusters_with_no_data, clusters_with_limited_data, split_percentage, exponential_fit_line_x, exponential_fit_line_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5560f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_polynomial_regression(waveform_biomass_data_file):\n",
    "    waveform_biomass_data = pd.read_csv(waveform_biomass_data_file)\n",
    "\n",
    "    # DF Variables\n",
    "    waveforms_with_biomass = waveform_biomass_data.iloc[:, 10:]\n",
    "    waveform_elevation_data = waveforms_with_biomass.to_numpy()\n",
    "\n",
    "    biomass_values = np.array(waveform_biomass_data[\"Biomass\"])\n",
    "    canopy_bottom_values = waveform_biomass_data[\"CanopyBottom\"].to_numpy()\n",
    "    surface_elevation_values = waveform_biomass_data[\"SurfaceElevation\"].to_numpy()\n",
    "\n",
    "    # Cluster vars\n",
    "    number_of_clusters = 10\n",
    "    \n",
    "    # Get metrics\n",
    "    waveform_elevation_RH10 = waveform_elevation_data[:, [5]]\n",
    "    waveform_elevation_RH975 = waveform_elevation_data[:, [-2]]\n",
    "\n",
    "    waveform_RH_start = canopy_bottom_values - surface_elevation_values\n",
    "\n",
    "    # Filter out waveforms with negative canopy heights\n",
    "    non_negative_mask = waveform_RH_start >= 0\n",
    "    waveform_RH_start_filtered = waveform_RH_start[non_negative_mask]\n",
    "    waveform_elevation_RH10_filtered = waveform_elevation_RH10[non_negative_mask, :]\n",
    "    waveform_elevation_RH975_filtered = waveform_elevation_RH975[non_negative_mask, :]\n",
    "    \n",
    "    biomass_values_masked = biomass_values[non_negative_mask]\n",
    "    waveform_RH_start_column = waveform_RH_start_filtered.reshape(-1, 1)\n",
    "    waveform_canopy_heights_RH975 = waveform_RH_start_column + waveform_elevation_RH975_filtered\n",
    "    waveform_canopy_heights_RH10 = waveform_RH_start_column + waveform_elevation_RH10_filtered\n",
    "\n",
    "    # Polynomial regression with degree 4\n",
    "    poly_feat = PolynomialFeatures(degree = 4)\n",
    "    X_poly = poly_feat.fit_transform(waveform_canopy_heights_RH975)\n",
    "\n",
    "    reg_model = LinearRegression().fit(X_poly, biomass_values_masked)\n",
    "    poly_regression_predictions = reg_model.predict(X_poly)\n",
    "    poly_rmse = np.sqrt(mean_squared_error(biomass_values_masked, poly_regression_predictions))\n",
    "\n",
    "    # Sort for graphing\n",
    "    sorted_indices = np.argsort(waveform_canopy_heights_RH975.flatten())\n",
    "    sorted_waveform_canopy_heights = waveform_canopy_heights_RH975.flatten()[sorted_indices]\n",
    "    sorted_poly_regression_predictions = poly_regression_predictions[sorted_indices]\n",
    "    \n",
    "    fig, (ax1) = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    ax1.set_title(f'Waveform Relative Height 97.5 and Exponential Relationship Plotted')\n",
    "    ax1.set_xlabel('Relative Height (m)')\n",
    "    ax1.set_ylabel('Biomass (kg)')\n",
    "    ax1.scatter(waveform_canopy_heights_RH975, biomass_values_masked)\n",
    "    ax1.plot(sorted_waveform_canopy_heights, sorted_poly_regression_predictions, color='red')\n",
    "\n",
    "    print(f\"RMSE: {poly_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f84ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_waveform_analysis_main(waveform_file, waveform_start_index, waveform_finish_index, biomass_group_num, mapping_tagging_table, apparent_individual_table):\n",
    "    '''Main waveform processing function to save waveform and biomass into a csv file compatible for regression analysis and clustering functions.\n",
    "       Returns a csv file with columns of NEON file number, Pulsenumber, in-situ biomass number associated with waveform, easting, northing, biomass group number\n",
    "                                                            dummy cluster number, and relative elevation at cumulative intensity intervals of 0.025'''\n",
    "    # Reading files vars\n",
    "    waveform_files = waveform_file.split('.')\n",
    "    pls_file = waveform_files[0] + '.pls'\n",
    "    wvs_file = waveform_files[0] + '.wvs'\n",
    "    readbin_pls_file = open(pls_file,\"rb\")\n",
    "    readbin_pls_file.seek(0,0)\n",
    "    readbin_wvs_file = open(wvs_file,\"rb\")\n",
    "    readbin_wvs_file.seek(0,0)\n",
    "    waveform_file_name = waveform_file.split(\"_\")[4]\n",
    "    \n",
    "    # Easting, northing, and resolution vars\n",
    "    min_easting_range = float('inf')\n",
    "    max_easting_range = float('-inf')\n",
    "    min_northing_range = float('inf')\n",
    "    max_northing_range = float('-inf')\n",
    "    distances_between_waveform_easting = []\n",
    "    distances_between_waveform_northing = []\n",
    "    waveform_easting_values = []\n",
    "    waveform_northing_values = []\n",
    "    prev_waveform_easting = 0\n",
    "    prev_waveform_northing = 0\n",
    "\n",
    "    # Waveform vars - change resolution if necessary\n",
    "    waveform_cum_energy = []\n",
    "    original_waveforms_intensity = []\n",
    "    original_waveforms_elevation = []\n",
    "    waveform_resolution = 1.5\n",
    "\n",
    "    # Biomass vars\n",
    "    biomass_resolution = 1\n",
    "    count = 0\n",
    "    shot_numbers_with_existing_biomass = []\n",
    "    waveform_easting_biomass_found = []\n",
    "    waveform_northing_biomass_found = []\n",
    "    total_biomass_nums = []\n",
    "\n",
    "    # Hardcoded biomass geolocation groups: Easting min, easting max, northing min, northing max\n",
    "    # Will differ based on NEON site - currently for Harvard Forest data\n",
    "    group_1_geolocation = [732106, 732147, 4713825, 4713844]\n",
    "    group_2_geolocation = [723926, 723944, 4707713, 4707732]\n",
    "    group_3_geolocation = [727586, 727605, 4704386, 4704405]\n",
    "    group_4_geolocation = [727077, 727097, 4704083, 4704104]\n",
    "    group_5_geolocation = [728124, 728143, 4702855, 4702877]\n",
    "    group_6_geolocation = [727049, 727066, 4701145, 4701163]\n",
    "    group_7_geolocation = [727884, 727902, 4700908, 4700925]\n",
    "    group_8_geolocation = [725546, 725566, 4700513, 4700534]\n",
    "    group_9_geolocation = [726500, 726521, 4699044, 4699063]\n",
    "    group_10_geolocation = [725997, 726016, 4698177, 4698195]\n",
    "    group_11_geolocation = [725964, 725986, 4696973, 4696994]\n",
    "    all_grouped_biomass_locations = np.array([group_1_geolocation,\n",
    "                                    group_2_geolocation,\n",
    "                                    group_3_geolocation,\n",
    "                                    group_4_geolocation,\n",
    "                                    group_5_geolocation,\n",
    "                                    group_6_geolocation,\n",
    "                                    group_7_geolocation,\n",
    "                                    group_8_geolocation,\n",
    "                                    group_9_geolocation,\n",
    "                                    group_10_geolocation,\n",
    "                                    group_11_geolocation])\n",
    "\n",
    "    # Save biomass geolocation into csv - only necessary if you want to view locations in QGIS\n",
    "    # biomass_df = pd.DataFrame({'Easting': biomass_easting_arr, 'Northing': biomass_northing_arr})\n",
    "    # biomass_df.to_csv(\"valid_biomass_geolocation_info.csv\", index=False)\n",
    "    \n",
    "    # Save waveform data into main csv file\n",
    "    waveform_elevation_axis_numbers = np.round(np.arange(0, 1.01, 0.025), 3)\n",
    "    columns = ['FileName', 'Pulsenumber', 'Biomass', 'Easting', 'Northing', 'Group', 'Cluster'] + [f\"Elevation_{i}\" for i in waveform_elevation_axis_numbers]\n",
    "    biomass_waveform_df = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    start_t = time.time()\n",
    "    # Get biomass geolocation information\n",
    "    biomass_northing_arr, biomass_easting_arr, biomass_id_arr, biomass_count = print_apparent_individual_biomass_information(apparent_individual_table, mapping_tagging_table, biomass_resolution, False)\n",
    "\n",
    "    # Save waveform and biomass data into main csv file\n",
    "    for iPulse in waveform_aoi:\n",
    "        dxdydz = dxdydz_array[iPulse,:]\n",
    "        xyz_bin0 = xyz_bin0_array[iPulse,:]\n",
    "        xyz_lastbin = xyz_lastbin_array[iPulse,:]\n",
    "        beam_name = beam_name_array[iPulse]\n",
    "        iPulse_pos = iPulse_pos_array[iPulse]\n",
    "\n",
    "        # Read waveform\n",
    "        waveform, waveform_x_axis, waveform_y_axis, waveform_z_axis, offset, stdev = get_level1_lidar_waveform(lidar_sensor_name,instrument_name,lidar_instrument_name,waveform_file,iPulse_pos,offset_to_pulse_data,pulse_size,T_scale_factor,T_offset,x_scale_factor,x_offset,y_scale_factor,y_offset,z_scale_factor,z_offset,sampling_record_pulse_descriptor_index_lookup_array,pulse_descriptor_optical_center_to_anchor_point_array,pulse_descriptor_number_of_extra_wave_bytes_array,pulse_descriptor_number_of_samplings_array,sampling_record_bits_for_duration_from_anchor_array,sampling_record_scale_for_duration_from_anchor_array,sampling_record_offset_for_duration_from_anchor_array,sampling_record_bits_for_number_of_segments_array,sampling_record_bits_for_number_of_samples_array,sampling_record_number_of_segments_array,sampling_record_number_of_samples_array,sampling_record_bits_per_sample_array,dxdydz,xyz_bin0,xyz_lastbin,beam_name,iPulse_pos)\n",
    "        cur_waveform_avg_easting = np.mean(waveform_x_axis)\n",
    "        cur_waveform_avg_northing = np.mean(waveform_y_axis)\n",
    "\n",
    "        return_location_count, return_peak_location_list, return_location_list_x, return_intensity_list = waveform_peak_detection(waveform,detection_threshold)\n",
    "\n",
    "        # Calculate distance between waveform for resolution\n",
    "        if count != 0:\n",
    "            distance_between_waveform_easting = abs(cur_waveform_avg_easting - prev_waveform_easting)\n",
    "            distance_between_waveform_northing = abs(cur_waveform_avg_northing - prev_waveform_northing)\n",
    "            distances_between_waveform_easting.append(distance_between_waveform_easting)\n",
    "            distances_between_waveform_northing.append(distance_between_waveform_northing)\n",
    "\n",
    "        waveform_min_easting = waveform_x_axis.min()\n",
    "        waveform_max_easting = waveform_x_axis.max()\n",
    "        waveform_min_northing = waveform_y_axis.min()\n",
    "        waveform_max_northing = waveform_y_axis.max()\n",
    "        indv_waveform_easting_geolocation = np.mean([waveform_min_easting, waveform_max_easting])\n",
    "        indv_waveform_northing_geolocation = np.mean([waveform_min_northing, waveform_max_northing])\n",
    "        \n",
    "        min_easting_range = min(min_easting_range, waveform_min_easting)\n",
    "        max_easting_range = max(max_easting_range, waveform_max_easting)\n",
    "        min_northing_range = min(min_northing_range, waveform_min_northing)\n",
    "        max_northing_range = max(max_northing_range, waveform_max_northing)\n",
    "\n",
    "        # Normalize waveform energy\n",
    "        waveform_energy_elev_arr = normalize_cumulative_return_energy(waveform, waveform_z_axis, False)\n",
    "        original_waveforms_intensity.append(waveform)\n",
    "        original_waveforms_elevation.append(waveform_z_axis)\n",
    "        waveform_cum_energy.append(waveform_energy_elev_arr)\n",
    "\n",
    "        # Find associated biomass value for waveform if it exists\n",
    "        # Can run find_associated_biomass_waveform_group if you just want to identify which waveforms have biomass\n",
    "        # biomass_group_found, biomass_group_number = find_associated_biomass_waveform_group(waveform_x_axis, waveform_y_axis, all_grouped_biomass_locations, waveform_start_index)\n",
    "        biomass_found, biomass_waveform_easting, biomass_waveform_northing = find_associated_biomass_waveform_pulse_number(biomass_northing_arr, biomass_easting_arr, waveform_x_axis, waveform_y_axis, waveform_start_index, False)\n",
    "        if biomass_found:\n",
    "            shot_numbers_with_existing_biomass.append(iPulse_pos)\n",
    "            waveform_easting_biomass_found.append(biomass_waveform_easting)\n",
    "            waveform_northing_biomass_found.append(biomass_waveform_northing)\n",
    "            waveform_easting_values.append(indv_waveform_easting_geolocation)\n",
    "            waveform_northing_values.append(indv_waveform_northing_geolocation)\n",
    "\n",
    "            # This is for a variable resolution (smaller). For now keep it hardcoded\n",
    "            # waveform_resolution = max(waveform_max_easting - waveform_min_easting, waveform_max_northing - waveform_min_northing)\n",
    "            # print(waveform_resolution)\n",
    "            biomass_num = calculate_total_tree_biomass(indv_waveform_northing_geolocation, indv_waveform_easting_geolocation, waveform_resolution, mapping_tagging_table, apparent_individual_table)\n",
    "\n",
    "            if biomass_num < 1.0:\n",
    "                print(f\"No biomass found for waveform {iPulse_pos}\")\n",
    "                continue\n",
    "            total_biomass_nums.append(biomass_num)\n",
    "            \n",
    "            # Interpolate waveform data for RH metrics\n",
    "            interpolated_wf, outlier_bool = interpolate_waveform_values(waveform_energy_elev_arr, False)\n",
    "            new_biomass_row = add_waveform_data_row_with_biomass(waveform_file_name, interpolated_wf, indv_waveform_easting_geolocation, indv_waveform_northing_geolocation, biomass_num, 0, waveform_start_index, biomass_group_num)\n",
    "            \n",
    "            # Add information to main dataframe\n",
    "            biomass_waveform_df = pd.concat([biomass_waveform_df, new_biomass_row], ignore_index=True)\n",
    "            \n",
    "        prev_waveform_easting = cur_waveform_avg_easting\n",
    "        prev_waveform_northing = cur_waveform_avg_northing\n",
    "        count += 1\n",
    "    end_t = time.time()\n",
    "    \n",
    "    print(f\"Processing biomass times: {end_t - start_t}\")\n",
    "    # print(shot_numbers_with_existing_biomass)\n",
    "    print(total_biomass_nums)\n",
    "\n",
    "    # Save geolocation of waveforms\n",
    "    # waveform_location_df = pd.DataFrame({'Pulsenumber': waveform_numbers, 'Easting': waveform_easting_values, 'Northing': waveform_northing_values})\n",
    "    # waveform_location_df.to_csv(\"waveform_geolocation_info.csv\", index=False)\n",
    "    \n",
    "    # Save main biomass waveform data - UNCOMMENT to save data\n",
    "    # biomass_waveform_df.to_csv(f\"biomass_waveform_data_main_group_{biomass_group_num}_{waveform_file_name}_copy.csv\", index=False)\n",
    "    \n",
    "    # Range of all the waveforms\n",
    "    print(f\"Easting range: {min_easting_range} to {max_easting_range}\")\n",
    "    print(f\"Northing range: {min_northing_range} to {max_northing_range}\")\n",
    "    print(f\"Average easting resolution between waveforms: {np.mean(distances_between_waveform_easting)}\")\n",
    "    print(f\"Average northing resolution between waveforms: {np.mean(distances_between_waveform_northing)}\")\n",
    "    print(f\"Max easting resolution between waveforms: {np.max(distances_between_waveform_easting)}\")\n",
    "    print(f\"Max northing resolution between waveforms: {np.max(distances_between_waveform_northing)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fb2fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_waveform_height_data(wave_file, start_index, end_index, waveform_nums):\n",
    "    '''Save last and first waveform height data points from NEON vegetation structure to a .csv with format:\n",
    "                                                                        PulseNumber, CanopyBottom, CanopyTop'''\n",
    "    \n",
    "    waveform_files = wave_file.split('.')\n",
    "    pls_file = waveform_files[0] + '.pls'\n",
    "    wvs_file = waveform_files[0] + '.wvs'\n",
    "    readbin_pls_file = open(pls_file,\"rb\")\n",
    "    readbin_pls_file.seek(0,0)\n",
    "    readbin_wvs_file = open(wvs_file,\"rb\")\n",
    "    readbin_wvs_file.seek(0,0)\n",
    "    waveform_file_name = wave_file.split(\"_\")[4]\n",
    "    print(waveform_file_name)\n",
    "    waveform_height_df = pd.DataFrame(columns=[\"PulseNumber\", \"CanopyBottom\", \"CanopyTop\"])\n",
    "    \n",
    "    instrument_name, number_of_pulses, xyz_origin_array, dxdydz_array, xyz_bin0_array, xyz_lastbin_array, \\\n",
    "    offset_to_pulse_data, pulse_size, T_scale_factor, T_offset, x_scale_factor, x_offset, y_scale_factor, y_offset, \\\n",
    "    z_scale_factor, z_offset, sampling_record_pulse_descriptor_index_lookup_array, pulse_descriptor_optical_center_to_anchor_point_array, \\\n",
    "    pulse_descriptor_number_of_extra_wave_bytes_array, pulse_descriptor_number_of_samplings_array, \\\n",
    "    sampling_record_bits_for_duration_from_anchor_array, sampling_record_scale_for_duration_from_anchor_array, \\\n",
    "    sampling_record_offset_for_duration_from_anchor_array, sampling_record_bits_for_number_of_segments_array, \\\n",
    "    sampling_record_bits_for_number_of_samples_array, sampling_record_number_of_segments_array, \\\n",
    "    sampling_record_number_of_samples_array, sampling_record_bits_per_sample_array = read_NEONAOP_pulsewaves_pulse_information(pls_file) \n",
    "\n",
    "    while start_index <= end_index:\n",
    "        if start_index in waveform_nums:\n",
    "            waveform, waveform_x_axis, waveform_y_axis, waveform_z_axis, offset, multiple_segments_bool = read_NEONAOP_pulsewaves_waveform(readbin_pls_file,readbin_wvs_file,start_index,offset_to_pulse_data,pulse_size,T_scale_factor,T_offset,x_scale_factor,x_offset,y_scale_factor,y_offset,z_scale_factor,z_offset,sampling_record_pulse_descriptor_index_lookup_array,pulse_descriptor_optical_center_to_anchor_point_array,pulse_descriptor_number_of_extra_wave_bytes_array,pulse_descriptor_number_of_samplings_array,sampling_record_bits_for_duration_from_anchor_array,sampling_record_scale_for_duration_from_anchor_array,sampling_record_offset_for_duration_from_anchor_array,sampling_record_bits_for_number_of_segments_array,sampling_record_bits_for_number_of_samples_array,sampling_record_number_of_segments_array,sampling_record_number_of_samples_array,sampling_record_bits_per_sample_array)\n",
    "\n",
    "            canopy_top = waveform_z_axis[0]\n",
    "            canopy_bottom = waveform_z_axis[-1]\n",
    "            waveform_height_df.loc[len(waveform_height_df)] = [start_index, canopy_bottom, canopy_top]\n",
    "        start_index += 1 \n",
    "            \n",
    "    waveform_height_df.to_csv(f\"waveform_elevations_{waveform_file_name}.csv\", index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d351f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dtm_surface_elevations(waveform_file_df, dtm_list, dtm_folder):\n",
    "    '''Get surface elevations from DTM files for each waveform based on geolocation data in waveform csv'''\n",
    "    \n",
    "    pulse_numbers = []\n",
    "    surface_elevation_numbers = []\n",
    "    canopy_heights = []\n",
    "    \n",
    "    for row in waveform_file_df.itertuples():\n",
    "        easting = row.Easting\n",
    "        northing = row.Northing\n",
    "        pulsenumber = row.Pulsenumber\n",
    "\n",
    "        for file in dtm_list:\n",
    "            file_easting = int(file.split(\"_\")[4])\n",
    "            file_northing = int(file.split(\"_\")[5])\n",
    "\n",
    "            easting_rounded = int(math.floor(easting / 1000) * 1000)\n",
    "            northing_rounded = int(math.floor(northing / 1000) * 1000)\n",
    "\n",
    "            if easting_rounded == file_easting and northing_rounded == file_northing:\n",
    "                xds = rxr.open_rasterio(dtm_folder + file)\n",
    "                surface_elevation = xds.interp(x=easting, y=northing, band=1).values\n",
    "\n",
    "                pulse_numbers.append(pulsenumber)\n",
    "                surface_elevation_numbers.append(surface_elevation)\n",
    "                \n",
    "                \n",
    "    return pulse_numbers, surface_elevation_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ba378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumulative_return_curves(waveform_pulsewaves_start_arr, waveform_pulsewaves_end_arr, pls_file, wvs_file):\n",
    "    '''K-means cluster helper function'''\n",
    "\n",
    "    min_easting_range = float('inf')\n",
    "    max_easting_range = float('-inf')\n",
    "    min_northing_range = float('inf')\n",
    "    max_northing_range = float('-inf')\n",
    "    distances_between_waveform_easting = []\n",
    "    distances_between_waveform_northing = []\n",
    "    avg_waveform_distance_easting = 0\n",
    "    avg_waveform_distance_northing = 0\n",
    "    prev_waveform_easting = 0\n",
    "    prev_waveform_northing = 0\n",
    "\n",
    "    pulsewaves_start = waveform_pulsewaves_start_arr[0]\n",
    "    current_pulsewaves_batch_counter = 0  \n",
    "    \n",
    "    readbin_pls_file = open(pls_file, \"rb\")\n",
    "    readbin_pls_file.seek(0, 0)\n",
    "\n",
    "    readbin_wvs_file = open(wvs_file, \"rb\")\n",
    "    readbin_wvs_file.seek(0, 0)\n",
    "    \n",
    "    waveform_cum_energy = []\n",
    "\n",
    "    # Read PLS file\n",
    "    instrument_name, number_of_pulses, xyz_origin_array, dxdydz_array, xyz_bin0_array, xyz_lastbin_array, \\\n",
    "    offset_to_pulse_data, pulse_size, T_scale_factor, T_offset, x_scale_factor, x_offset, y_scale_factor, y_offset, \\\n",
    "    z_scale_factor, z_offset, sampling_record_pulse_descriptor_index_lookup_array, pulse_descriptor_optical_center_to_anchor_point_array, \\\n",
    "    pulse_descriptor_number_of_extra_wave_bytes_array, pulse_descriptor_number_of_samplings_array, \\\n",
    "    sampling_record_bits_for_duration_from_anchor_array, sampling_record_scale_for_duration_from_anchor_array, \\\n",
    "    sampling_record_offset_for_duration_from_anchor_array, sampling_record_bits_for_number_of_segments_array, \\\n",
    "    sampling_record_bits_for_number_of_samples_array, sampling_record_number_of_segments_array, \\\n",
    "    sampling_record_number_of_samples_array, sampling_record_bits_per_sample_array = read_NEONAOP_pulsewaves_pulse_information(pls_file)\n",
    "\n",
    "    loop_count = 0\n",
    "    while pulsewaves_start < waveform_pulsewaves_end_arr[len(waveform_pulsewaves_end_arr)-1]:\n",
    "        \n",
    "        # Read waveform\n",
    "        waveform, waveform_x_axis, waveform_y_axis, waveform_z_axis, offset = read_NEONAOP_pulsewaves_waveform(readbin_pls_file,readbin_wvs_file,instrument_name,lidar_instrument_name,pulsewaves_start,offset_to_pulse_data,pulse_size,T_scale_factor,T_offset,x_scale_factor,x_offset,y_scale_factor,y_offset,z_scale_factor,z_offset,sampling_record_pulse_descriptor_index_lookup_array,pulse_descriptor_optical_center_to_anchor_point_array,pulse_descriptor_number_of_extra_wave_bytes_array,pulse_descriptor_number_of_samplings_array,sampling_record_bits_for_duration_from_anchor_array,sampling_record_scale_for_duration_from_anchor_array,sampling_record_offset_for_duration_from_anchor_array,sampling_record_bits_for_number_of_segments_array,sampling_record_bits_for_number_of_samples_array,sampling_record_number_of_segments_array,sampling_record_number_of_samples_array,sampling_record_bits_per_sample_array)\n",
    "        cur_waveform_avg_easting = np.mean(waveform_x_axis)\n",
    "        cur_waveform_avg_northing = np.mean(waveform_y_axis)\n",
    "    \n",
    "        # Calculate distance between waveform for resolution\n",
    "        if loop_count != 0:\n",
    "            distance_between_waveform_easting = abs(cur_waveform_avg_easting - prev_waveform_easting)\n",
    "            distance_between_waveform_northing = abs(cur_waveform_avg_northing - prev_waveform_northing)\n",
    "            distances_between_waveform_easting.append(distance_between_waveform_easting)\n",
    "            distances_between_waveform_northing.append(distance_between_waveform_northing)\n",
    "\n",
    "            prev_waveform_easting = cur_waveform_avg_easting\n",
    "            prev_waveform_northing = cur_waveform_avg_northing\n",
    "    \n",
    "        waveform_min_easting = waveform_x_axis.min()\n",
    "        waveform_max_easting = waveform_x_axis.max()\n",
    "        waveform_min_northing = waveform_y_axis.min()\n",
    "        waveform_max_northing = waveform_y_axis.max()\n",
    "        \n",
    "        min_easting_range = min(min_easting_range, waveform_min_easting)\n",
    "        max_easting_range = max(max_easting_range, waveform_max_easting)\n",
    "        min_northing_range = min(min_northing_range, waveform_min_northing)\n",
    "        max_northing_range = max(max_northing_range, waveform_max_northing)\n",
    "        \n",
    "        waveform_energy_elev_arr = normalize_cumulative_return_energy(waveform, waveform_z_axis, False)\n",
    "        \n",
    "        # Interesting observation: length is either 80, 96, 112, or 240 (multiple segments in 240) so 40, 48, 56, or 120 values\n",
    "        waveform_cum_energy.append(waveform_energy_elev_arr)\n",
    "        if (current_pulsewaves_batch_counter + 1) < len(waveform_pulsewaves_end_arr) and pulsewaves_start == waveform_pulsewaves_end_arr[current_pulsewaves_batch_counter]:\n",
    "            current_pulsewaves_batch_counter += 1\n",
    "            pulsewaves_start = waveform_pulsewaves_start_arr[current_pulsewaves_batch_counter]\n",
    "        else:\n",
    "            pulsewaves_start += 1\n",
    "            loop_count += 1\n",
    "            \n",
    "    print(pls_file)\n",
    "    print(f\"Easting range: {min_easting_range} to {max_easting_range}\")\n",
    "    print(f\"Northing range: {min_northing_range} to {max_northing_range}\")\n",
    "    print(f\"Average easting resolution between waveforms: {np.mean(distances_between_waveform_easting)}\")\n",
    "    print(f\"Average northing resolution between waveforms: {np.mean(distances_between_waveform_northing)}\")\n",
    "    print(f\"Max easting resolution between waveforms: {np.max(distances_between_waveform_easting)}\")\n",
    "    print(f\"Max northing resolution between waveforms: {np.max(distances_between_waveform_northing)}\")\n",
    "    print(f\"Total size of energy arr: {len(waveform_cum_energy)}\")\n",
    "    return waveform_cum_energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4399df1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "K_means_cluster execution and setup with all waveforms\n",
    "Cluster 500,000 waveforms with associated biomass values within these clusters\n",
    "Input assumption: Input csv file with waveform data is sorted such that all waveforms from the same file are contiguous\n",
    "Output: Cluster assignment for each waveform sorted by pulsenumbers in input csv file order\n",
    "'''\n",
    "\n",
    "waveform_start_L022 = 3580000\n",
    "waveform_end_L022 = 3680000\n",
    "\n",
    "waveform_start_L018 = 4900000\n",
    "waveform_end_L018 = 5000000\n",
    "waveform_start_L018_2 = 7080000\n",
    "waveform_end_L018_2 = 7180000\n",
    "\n",
    "waveform_start_L019 = 4700000\n",
    "waveform_end_L019 = 4800000\n",
    "waveform_start_L019_2 = 8310000\n",
    "waveform_end_L019_2 = 8410000\n",
    "\n",
    "waveform_file_L022 = '/Users/felixyu/Documents/Remote-Sensing/Neon-Pls-Wvs-Las/data/NEON_D01_HARV_DP1_L022-1_2019082013_1.pls'.split('.')\n",
    "waveform_file_L019 = '/Users/felixyu/Documents/Remote-Sensing/Neon-Pls-Wvs-Las/data/NEON_D01_HARV_DP1_L019-1_2019082013_1.pls'.split('.')\n",
    "waveform_file_L018 = '/Users/felixyu/Documents/Remote-Sensing/Neon-Pls-Wvs-Las/data/NEON_D01_HARV_DP1_L018-1_2019082013_1.pls'.split('.')\n",
    "\n",
    "pls_file_L022 = waveform_file_L022[0] + '.pls'\n",
    "wvs_file_L022 = waveform_file_L022[0] + '.wvs'\n",
    "pls_file_L019 = waveform_file_L019[0] + '.pls'\n",
    "wvs_file_L019 = waveform_file_L019[0] + '.wvs'\n",
    "pls_file_L018 = waveform_file_L018[0] + '.pls'\n",
    "wvs_file_L018 = waveform_file_L018[0] + '.wvs'\n",
    "\n",
    "k_list = [10, 15]\n",
    "\n",
    "biomass_waveforms_df = pd.read_csv(\"/Users/felixyu/Documents/Remote-Sensing/Neon-Pls-Wvs-Las/data/biomass_waveform_data_main.csv\")\n",
    "shot_numbers_with_biomass = biomass_waveforms_df[\"Pulsenumber\"].to_numpy()\n",
    "associated_file_names = biomass_waveforms_df[\"FileName\"].to_numpy()\n",
    "\n",
    "waveforms_with_biomass_interpolated = []\n",
    "interpolated_waveforms = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "cumulative_return_curves_L022 = get_cumulative_return_curves([waveform_start_L022], [waveform_end_L022], pls_file_L022, wvs_file_L022)\n",
    "cumulative_return_curves_L018 = get_cumulative_return_curves([waveform_start_L018, waveform_start_L018_2], [waveform_end_L018, waveform_end_L018_2], pls_file_L018, wvs_file_L018)\n",
    "cumulative_return_curves_L019 = get_cumulative_return_curves([waveform_start_L019, waveform_start_L019_2], [waveform_end_L019, waveform_end_L019_2], pls_file_L019, wvs_file_L019)\n",
    "\n",
    "aggregated_cumulative_return_curves = cumulative_return_curves_L022 + cumulative_return_curves_L018 + cumulative_return_curves_L019\n",
    "\n",
    "for waveform in aggregated_cumulative_return_curves:\n",
    "    interpolated_wf_one, outlier = interpolate_waveform_values(waveform, False)\n",
    "\n",
    "    if not outlier:\n",
    "        interpolated_waveforms.append(interpolated_wf_one)\n",
    "interpolated_waveforms = np.array(interpolated_waveforms)\n",
    "\n",
    "# Get interpolated waveforms of only biomass data\n",
    "waveforms_intensity_data = np.round(np.arange(0.0, 1.01, 0.025), 3)\n",
    "waveforms_with_biomass = biomass_waveforms_df.iloc[:, 10:]\n",
    "elevations_with_biomass_aggregated = (waveforms_with_biomass.apply(lambda row: np.array(row), axis=1)).to_numpy()\n",
    "\n",
    "# Add intensity data with each elevation array\n",
    "for elevation_arr in elevations_with_biomass_aggregated:\n",
    "    waveform_with_biomass_interpolated = np.stack((waveforms_intensity_data, elevation_arr), axis=1).ravel()\n",
    "    waveforms_with_biomass_interpolated.append(waveform_with_biomass_interpolated)\n",
    "\n",
    "k_means_classifiers = train_k_means_cluster_cumulative_returns(interpolated_waveforms, k_list, True)\n",
    "cluster_centers, cluster_assignments = test_k_means_cluster_cumulative_returns(k_means_classifiers, waveforms_with_biomass_interpolated, k_list, True, True)\n",
    "\n",
    "# Plot cluster centers vs full raw data for each k value\n",
    "count = 0\n",
    "for i, cluster_group in enumerate(cluster_centers):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    \n",
    "    for waveform in interpolated_waveforms:\n",
    "        plt.plot(waveform[0::2], waveform[1::2], color='lightgray', alpha=0.7)\n",
    "        count += 1\n",
    "        if count > 5000:\n",
    "            count = 0\n",
    "            break\n",
    "\n",
    "    for cluster in cluster_group:\n",
    "        clustered_signal_levels = cluster[0::2]\n",
    "        clustered_elevation = cluster[1::2]\n",
    "        plt.plot(clustered_signal_levels, clustered_elevation, color='green')\n",
    "    \n",
    "    plt.title(f\"Interpolated Waveforms vs. Clustered Center Waveforms for {k_list[i]} Clusters\")\n",
    "    plt.xlabel(\"Normalized Cumulative Signal Level\")\n",
    "    plt.ylabel(\"Relative Elevation (m)\")\n",
    "    waveform_legend = mlines.Line2D([], [], color='lightgray', alpha=0.7, label='Waveforms')\n",
    "    cluster_legend = mlines.Line2D([], [], color='green', label='Clusters')\n",
    "    plt.legend(handles=[waveform_legend, cluster_legend])\n",
    "    plt.show()\n",
    "\n",
    "# Plot cluster centers vs test data for each k value, highlighted by which cluster they belong to \n",
    "for i, cluster_assignment in enumerate(cluster_assignments):\n",
    "    unique_clusters = np.unique(cluster_assignment)\n",
    "    # Create a colormap with as many colors as there are clusters.\n",
    "    colors = [\n",
    "        '#e6194b',  # Vivid Red\n",
    "        '#3cb44b',  # Kelly Green\n",
    "        '#ffe119',  # Bright Yellow\n",
    "        '#4363d8',  # Royal Blue\n",
    "        '#f58231',  # Orange\n",
    "        '#911eb4',  # Purple\n",
    "        '#46f0f0',  # Cyan/Aqua\n",
    "        '#f032e6',  # Magenta\n",
    "        '#bcf60c',  # Lime Green\n",
    "        '#fabebe',  # Light Pink\n",
    "        '#800000',  # Maroon\n",
    "        '#008080',  # Teal\n",
    "        '#808000',  # Olive\n",
    "        '#9A6324',  # Brown\n",
    "        '#000075',  # Navy\n",
    "    ]\n",
    "    \n",
    "    plt.figure(figsize=(8,8))\n",
    "\n",
    "    # Plot each waveform with its corresponding cluster color.\n",
    "    for waveform, cluster in zip(waveforms_with_biomass_interpolated, cluster_assignment):\n",
    "        plt.plot(waveform[0::2], waveform[1::2], color=colors[cluster], alpha=0.7)\n",
    "    \n",
    "    # Create legend handles for each cluster color.\n",
    "    cluster_handles = []\n",
    "    for cluster in unique_clusters:\n",
    "        handle = mlines.Line2D([], [], color=colors[cluster], label=f'Cluster {cluster}')\n",
    "        cluster_handles.append(handle)\n",
    "    \n",
    "    plt.title(f\"Biomass Waveforms Visualized Based on Cluster Centers for {k_list[i]} Clusters\")\n",
    "    plt.xlabel(\"Normalized Cumulative Signal Level\")\n",
    "    plt.ylabel(\"Relative Elevation (m)\")\n",
    "    plt.legend(handles=cluster_handles)\n",
    "    plt.show()\n",
    "\n",
    "# Save cluster assignments\n",
    "columns = ['FileName', 'Pulsenumber', 'Cluster']\n",
    "\n",
    "cluster_assignments_df_10 = pd.DataFrame(columns=columns)\n",
    "cluster_assignments_df_10 = cluster_assignments_df_10.astype({'FileName': 'string','Pulsenumber': 'int64','Cluster': 'int32'})\n",
    "cluster_assignments_df_15 = cluster_assignments_df_10.copy()\n",
    "\n",
    "for i, pulsenum in enumerate(shot_numbers_with_biomass):\n",
    "    new_cluster_row = {\n",
    "        'FileName': associated_file_names[i],\n",
    "        'Pulsenumber': pulsenum,\n",
    "        'Cluster': cluster_assignments[0][i]\n",
    "    }\n",
    "    cluster_assignments_df_10.loc[len(cluster_assignments_df_10)] = new_cluster_row\n",
    "\n",
    "for i, pulsenum in enumerate(shot_numbers_with_biomass):\n",
    "    new_cluster_row = {\n",
    "        'FileName': associated_file_names[i],\n",
    "        'Pulsenumber': pulsenum,\n",
    "        'Cluster': cluster_assignments[1][i]\n",
    "    }\n",
    "    cluster_assignments_df_15.loc[len(cluster_assignments_df_15)] = new_cluster_row\n",
    "    \n",
    "cluster_assignments_df_10.to_csv(\"waveform_biomass_cluster_assignments_k_10.csv\", index=False)\n",
    "cluster_assignments_df_15.to_csv(\"waveform_biomass_cluster_assignments_k_15.csv\", index=False)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Total processing time: {end-start}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
